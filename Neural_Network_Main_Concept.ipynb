{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<br/>\n",
    "\n",
    "해당 자료는 Multi Layer Perceptron이라고 불리는 기본적인 인공 신경망에 대해서 설명한 자료입니다.\n",
    "\n",
    "해당 자료는 다음과 같으신 분을 위해서 쓰여졌습니다.\n",
    "\n",
    "1. Python 기초를 아시는 분\n",
    "2. 기본적인 딥러닝의 이론에 대해서 이해가 있으신분. \n",
    "3. Tensorflow나 Torch등 기본적인 프레임워크로도 구현을 해봤는데, 내부 동작은 어떻게 돌아가는지에 대해서 궁금하신 분\n",
    "\n",
    "<br/>\n",
    "만약에 해당 스크립트가 너무 어렵게 느껴지시거나 이론을 조금 더 알고 싶으시다면 아래의 링크에서 딥러닝의 기초에 대해서 배울 수 있으니, 먼저 기본적인 딥러닝에 대해서 학습하고 오시기를 권장해드립니다\n",
    "\n",
    "<br/>\n",
    "[<font size=\"3\">[1]. 모두의 딥러닝 (무료/국문)](https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)\n",
    "<br/><br/>\n",
    "[<font size=\"3\">[2]. 헬로 딥러닝 (무료/국문)](https://www.youtube.com/watch?v=yWySw4EfSJc&t=424s)\n",
    "<br/><br/>\n",
    "[<font size=\"3\">[3]. 코세라, Deep Learning Specialization (유료/영문)](https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&campaignid=917423980&adgroupid=46295378339&device=c&keyword=coursera%20deep%20learning&matchtype=p&network=g&devicemodel=&adpostion=1t1&creativeid=217989182387&hide_mobile_promo&gclid=EAIaIQobChMI_cnop7HU1wIVzgMqCh2CdAQiEAAYASAAEgKbcfD_BwE)\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Neural Network :: Multi Layer Perceptron\n",
    "\n",
    "\n",
    "<br/>\n",
    "시작하기 앞서서 해당 스크립트의 핵심은 다음과 같이 구현되어 있습니다.\n",
    "\n",
    "- 기본적인 인공신경망(Multi Layer Perceptron)에 대해서 설명합니다.\n",
    "\n",
    "\n",
    "- <전파 알고리즘(Forward Operation)> / 역전파 알고리즘(BackPropagation) / 활성화함수(Activation function) / 분류기(Softamx) / 에러함수(MSE, Cross-Entropy Error) 에 대한 작은 모듈들을 만듭니다.\n",
    "\n",
    "\n",
    "- 해당 모듈을 만들고나면, 이를 연결하여 하나의 인공신경망을 만듭니다.\n",
    "\n",
    "\n",
    "- 이에 대한 결과를 확인합니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "**<font size=\"5\">목차**\n",
    "\n",
    "**<font size=\"3\">1. MNIST DataSet 설명 및 데이터 로드**<br/>\n",
    "\n",
    "**<font size=\"3\">2. Multi-Layer Perceptron Overview**\n",
    "    \n",
    "**<font size=\"3\">3. 전파 알고리즘(Forward Operation) 개념 :: 기초**\n",
    "    \n",
    "**<font size=\"3\">4. 전파 알고리즘(Forward Operation) 개념 :: 행렬**\n",
    "\n",
    "**<font size=\"3\">5. 활성화 함수(Activation Function) 개념**\n",
    "\n",
    "**<font size=\"3\">6. 비용 함수(Cost Function) 개념 :: 분류 오차/MSE/CEE**\n",
    "\n",
    "**<font size=\"3\">7. 경사 하강 알고리즘(Gradient Descent) 개념 **\n",
    "\n",
    "**<font size=\"3\">8. 역 전파 알고리즘(Backpropagation) 개념 **\n",
    "    \n",
    "**<font size=\"3\">9. 가중치(weights) 생성 및 초기화 :: 단일 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">10. 가중치(weights) 생성 및 초기화 :: 다중 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">11. 전파 알고리즘(Forward Operation) :: 단일 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">12. 활성화 함수(Activation Function)와 활성화 함수의 미분 (Code)**\n",
    "\n",
    "**<font size=\"3\">13. 다중 전파 알고리즘(Forward Operation) :: 다중 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">14. 비용 함수(Cost Function) (Code)**\n",
    "    \n",
    "**<font size=\"3\">15. 역 전파 알고리즘(Backpropagation) (Code)**\n",
    "    \n",
    "**<font size=\"3\">16. MNIST 학습 :: numpy 기반 (Code)**\n",
    "    \n",
    "**<font size=\"3\">17. MNIST 학습 :: tensorflow 기반 (Code)**\n",
    "\n",
    "<br/>\n",
    "<br/>*<font color=\"red\">해당 스크립트는 코세라의 Deep Learning Specialization의 코드를 기반으로 했음을 알려드립니다*\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - MNIST Data Load \n",
    "\n",
    "먼저, 예제 데이터를 입력으로 사용하기 위해서 MNIST 데이터를 로드하겠습니다.\n",
    "\n",
    "MNIST 데이터는 `Figure 1`과 같이 사람들의 다양한 손글씨 이미지를 대량으로 모아놓은 데이터입니다.\n",
    "<br/><br/>\n",
    "<img src=\"images/14481-mnist-dataset.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 1</u>: MNIST DataSet </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "일반적으로 모든 이미지는 데이터로 변환할 수 있는데, 아래의 Figure 2와 같이 하얀색에 가까울 수록 값이 0에 가까워지고\n",
    "검은색에 가까워질 수록 값이 1에 가까워지는 형태로 표현할 수 있습니다.\n",
    "<br/><br/>\n",
    "그림과 같이 MNIST 데이터는 다양한 손글씨 데이터를 width=28, height=28의 흑백이미지(검은색과 하얀색으로 이루어진)의 데이터 집합이라고 보시면 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "*<font color=\"red\">이해를 위해서 그림과 일치하게 설명한 것일 뿐이므로, 컨셉적으로만 이해하시면 됩니다.*<br/><br/>\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/d4e5709ebb4ba940126de44c76ca71b0.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 2</u>: Image Data </center></caption>\n",
    "\n",
    "\n",
    "MNIST 데이터의 특징은 다음과 같습니다.\n",
    "\n",
    "- 이미지의 width와 height가 모두 28입니다.\n",
    "\n",
    "\n",
    "- 흑백 이미지입니다.\n",
    "\n",
    "\n",
    "- 따라서 MNIST 데이터의 갯수는 $28x28x1=784$이 됩니다.<br/>\n",
    "*<font color=\"red\">이 또한 이해를 위해서 간략하게 설명한 것입니다. 지금은 이해하는 것이 중요하므로 그냥 그렇구나 넘어가시면 됩니다.*<br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## 2 - Multi Layer Perceptron :: Overview\n",
    "\n",
    "<br/><br/>\n",
    "인공 신경망은 생물학에서 뉴런으로부터 영감을 받아서 출발한 이론입니다.\n",
    "\n",
    "생물학에서 뉴런은 `Figure 3`의 왼쪽 하단 같이 생겼습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "생물학적 뉴런은 다음과 같은 행동을 합니다.\n",
    "\n",
    "- `다른 뉴런들로부터 신호를 받는다.`\n",
    "- `그 신호들을 종합한다.`\n",
    "- `그 종합한 결과를 통해서 다음 뉴런에게 신호를 보낼지 말지 결정한다.`\n",
    "<br/><br/>\n",
    "\n",
    "이러한 행동을 모델링한 것이 인공 신경망입니다.인공 신경도 똑같은 행동을 합니다.\n",
    "<br/>\n",
    "\n",
    "- `다른 뉴런들로부터 신호를 받는다.`\n",
    "- `그 신호들을 종합한다.`\n",
    "- `그 종합한 결과를 통해서 다음 뉴런에게 신호를 보낼지 말지 결정한다.`\n",
    "\n",
    "이것이 기본적인 인공 신경망의 컨셉입니다.\n",
    "\n",
    "이러한 기본적인 컨셉을 가지고 여러가지 인공 신경을 연결해 놓은 것을 인공 신경망이라고 합니다.\n",
    "\n",
    "이를 다른 말로는 MLP(Multi Layer Perceptron)이라고 합니다.\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"images/neuron.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> <u>Figure 3</u>: Biological Neuron & Artificial Neuron</center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "**<font size=\"4\">이제부터 본격적으로 인공신경망에 대해서 자세하게 살펴보겠습니다.**\n",
    "\n",
    "<br/><br/>\n",
    "MLP(Multi Layer Perceptron :: 이제부터 편의상 MLP라고 줄여서 이야기하도록 하겠습니다)불리는 인공신경망의 계산 순서는 다음과 같습니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "**<font size=\"3\">1.실행 :: 학습이 완료되었을 경우**<br/><br/>\n",
    "\n",
    "학습이 완료되었음을 가정했을 때, 인공신경망의 작동순서는 아래와 같이, 데이터를 입력하고,\n",
    "\n",
    "입력된 데이터는 전파 알고리즘을 통해서 인공신경망을 통과하게되고, 그 결과를 그대로 사용하게됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "- `데이터 입력`<br/>\n",
    "- `전파 알고리즘 (Forward Operation)`\n",
    "\n",
    "<img src=\"images/MLP_test.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> <u>Figure 4</u>: MLP Test</center></caption>\n",
    "\n",
    "\n",
    "**<font size=\"3\">2.학습**\n",
    "\n",
    "만약 인공신경망이 학습이 덜 되었다면, 인공신경망을 학습시키는 과정을 거쳐야합니다.\n",
    "\n",
    "학습과정은 아래와 같이 데이터를 입력하고, 전파 알고리즘을 통해서 얻은 결과와 정답사이의 오차를 측정해서\n",
    "\n",
    "해당 오차를 역전파 알고리즘으로 전파하여 가중치들의 값을 업데이트해주게 됩니다.\n",
    "\n",
    "위의 과정을 학습이 완료될 때 까지 반복해서 학습하게 됩니다.\n",
    "\n",
    "\n",
    "- `데이터 입력`\n",
    "- `전파 알고리즘 (Forward Operation)`\n",
    "- `비용 함수 계산 (Cost Calculation)`\n",
    "- `역전파 알고리즘 (Backpropagation)`\n",
    "- `위의 4가지 순서를 반복 (Iteration)`\n",
    "\n",
    "<img src=\"images/MLP_train.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> <u>Figure 5</u>: MLP Train</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "`Figure 6`은 그림에서 위의 알고리즘이 어디에 해당하는지 표시한 그림입니다.\n",
    "\n",
    "<img src=\"images/MLP_train_detail.png\" style=\"width:850px;height:400px;\">\n",
    "<caption><center> <u>Figure 6</u>: MLP Train Detail</center></caption>\n",
    "\n",
    "### 2-1 전파 알고리즘(Forward Operation) :: 기본 컨셉 (basic concept)\n",
    "\n",
    "<br/>\n",
    "\n",
    "앞에서 이야기했듯이 생물학적 신경망을 수학적으로 모델링한 것이 인공 신경망이라고 말씀 드렸습니다.\n",
    "\n",
    "그럼 이번 챕터에서는 다른 신경들로부터 받은 신호들을 어떻게 종합해서 결과를 출력하는지에 대해서 설명드리도록 하겠습니다.\n",
    "\n",
    "다른 신경들로부터 받은 신호를 종합하는 과정을 인공신경망에선느 전파 알고리즘(Forward Operation)이라고 합니다.\n",
    "\n",
    "이러한 전파 과정들에 대해서 나타낸 그림이 바로 `Figure 6`입니다\n",
    "\n",
    "`Figure 7`에 나타난 각 숫자들과 문자들에 대해서는 그림 아래에 Notation이 있습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"images/Multilayer-Perceptron-Forward-Propagation.png\" style=\"width:650px;height:550px;\">\n",
    "<caption><center> <u>Figure 7</u>: 1-Layer MLP </center></caption>\n",
    "\n",
    "\n",
    "**<font size=\"3\"><font color=\"red\">Note**: <br/><br/>\n",
    "*$l1,l2$* : 각각 하나의 입력데이터를 의미합니다.\n",
    "\n",
    "*화살표의 숫자* : 가중치 값을 의미합니다. 해당 스크립트에서는 $l1$에서 $H1$으로 가는 가중치를 $W_{l_{1}}^{H_{1}}$이라고 하겠습니다\n",
    "\n",
    "**$H1, H2, H3$** : 각각의 뉴런(Hidden Unit)입니다.\n",
    "\n",
    "**$O1, O2$** : 각각의 출력 뉴런을 의미합니다\n",
    "\n",
    "*활성화 함수(Activation function)* : 해당 그림에는 표기되어있지 않으나, 활성화 함수는 `Sigmoid`입니다.<br/>\n",
    "`Sigmoid`함수는 추후에 설명하겠으나, 흐름을 따라가기 위해서 기본적인 수식이 이렇다는 것만 알아두고 넘어가겠습니다.<br/>\n",
    "\n",
    "$$Sigmoid = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "이제부터 전파 알고리즘이 어떻게 진행되는지, 차례차레 풀어보도록 하겠습니다.\n",
    "\n",
    "기본적으로 인공 신경망의 연산은 다음과 같은 수식을 띕니다.\n",
    "\n",
    "<br/>\n",
    "$$Y = \\sigma(WX+b)$$\n",
    "\n",
    "<br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "\n",
    "수식이 어렵게 느껴지실 수도 있지만, 정말 간단합니다.\n",
    "\n",
    "$W$ : 가중치를 의미합니다.\n",
    "\n",
    "$X$ : 입력값을 의미합니다.\n",
    "\n",
    "$b$ : 편향(bias)라고 불리는 숫자(상수)입니다.\n",
    "\n",
    "$\\sigma$ : 활성화 함수(Acitvation function)을 의미합니다\n",
    "\n",
    "$Y$ : 출력값입니다.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "이를 가지고 말로 풀어서 써보자면, \n",
    "- `입력과 가중치를 곱합니다.`\n",
    "- `그 결과에 어떠한 숫자(편향)를 더합니다.`\n",
    "- `그 결과를 활성화 함수에 넣어줍니다.`\n",
    "- `활성화 함수의 결과가 출력값이 됩니다.`\n",
    "\n",
    "이 수식은 결국 맨 처음 이야기한 인공 신경이 하는 역활(다른 신경으로부터 신호를 받아서 종합하는 것)과 같다는 것을 알 수 있습니다.\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "**<font size=\"4\">1. $H1, H2, H3$ 뉴런 Input값과 Output값 유도**\n",
    "<br/><br/>\n",
    "그럼 이제 $H1$의 결과가 어떻게 나오게 됬는지 $Y = \\sigma(WX+b)$을 통해서 확인해보겠습니다.\n",
    "    \n",
    "<br/><br/>    \n",
    "\n",
    "- $Z = (l1 \\times 0.3) + (l2 \\times 0.2) + 0 = (10\\times0.3) + (20\\times0.2) + 0 = 7$ :: *<font color=\"red\">여기에서 bias는 없으므로 0으로 대체합니다.*\n",
    "\n",
    "\n",
    "- $Sigmoid(Z) = \\frac{1}{1+e^{-7}} = 0.999$\n",
    "<br/><br/>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이런 형태로 $H2, H3$ 그리고 더 나아가서 $O1, O2$값도 똑같은 방법으로 구할 수 있습니다.\n",
    "\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Forward Operation :: matrix concept\n",
    "<br/>\n",
    "지금까지 기본적인 $Y = \\sigma(WX+b)$를 이용해서, 어떻게 MLP의 전파 알고리즘이 진행되는지 확인하였습니다\n",
    "\n",
    "이번에는 이를 모두 하나의 행렬로 묶어서 표현하고 연산하는 방법에 대해서 소개하겠습니다.\n",
    "\n",
    "`Figure 6`의 값들은 모두 행렬로 표현할 수 있고, 행렬 연산으로 수행할 수 있습니다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**행렬 연산으로 푸는 이유는 프로그래밍으로 행렬연산은 병렬처리가 가능하며, 수학적으로도 수식 전체가 간소화되는 이점이 있기 때문입니다.**\n",
    "\n",
    "<br/><br/>\n",
    "먼저 입력에 대해서 먼저 보면 $l1 = 10,  l2 = 20$ 입니다. \n",
    "\n",
    "<br/>\n",
    "이를 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$Input = \\begin{bmatrix}\n",
    "    10 & 20\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "Input layer와 Hidden Layer 사이의 가중치들을 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$W_{input}^{hidden} = \\begin{bmatrix}\n",
    "    0.3  & -0.1  &  1.1 \\\\\n",
    "    0.2  & -0.2  &  -0.5\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "Hidden Layer와 Output layer 사이의 가중치들을 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$W_{hidden}^{out} = \\begin{bmatrix}\n",
    "    1.1  &  0.4 \\\\\n",
    "    0.5  &  0.3 \\\\\n",
    "    0.7  &  0.2\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "<br/><br/>\n",
    "이렇게 데이터들을 행렬로 표현하고, 위와 똑같이 $Y = \\sigma(WX+b)$를 적용하면 더 간편하게 표현할 수 있습니다.\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "    \n",
    "여기서의 곱셈 연산은 행렬곱셈 연산을 의미합니다.<br/>\n",
    "행렬 곱셈 연산은 다음 Figure 8와 같은 방식으로 이루어집니다.<br/>\n",
    "*<font color=\"red\">행렬안의 내용이 어떠한 내용인지는 모르셔도 됩니다. 단지 행렬곱이 어떠한 순서로 이루어지는지만 보시면 됩니다.*<br/><br/>\n",
    "<img src=\"images/animation03.gif\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 8</u>: Matrix Multiply</center></caption>\n",
    "\n",
    "\n",
    "    \n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\">1. $H1, H2, H3$ 뉴런값들을 행렬 연산으로 유도** <br/>\n",
    "    \n",
    "계속 이야기했듯이 행렬표현으로 변경해도 전파 알고리즘 수식인 $Y = \\sigma(WX+b)$가 똑같이 적용된다고 말씀드렸었습니다.\n",
    "\n",
    "그럼 지금부터 행렬 연산을 기반으로 $H1, H2, H3$ 뉴런값을 한번에 구해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "- $Z=WX+b$<br/><br/>\n",
    "$Z_{hidden} = \\begin{bmatrix}H1&H2&H3\\end{bmatrix} = \\begin{bmatrix}10&20\\end{bmatrix}\\times\\begin{bmatrix}0.3&-0.1&1.1\\\\0.2&-0.2&-0.5\\end{bmatrix} + \\begin{bmatrix}0&0&0\\end{bmatrix} = \\begin{bmatrix}7&-5&1\\end{bmatrix}$<br/><br/><br/>\n",
    "\n",
    "- $Y_{hidden} = Sigmoid(Z_{hidden})$<br/><br/>\n",
    "$ Y= \\begin{bmatrix}Sigmoid(H1)&Sigmoid(H2)&Sigmoid(H3)\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{1+e^{-7}}&\\frac{1}{1+e^{5}}&\\frac{1}{1+e^{-1}}\\end{bmatrix} = \\begin{bmatrix}0.999&0.007&0.731\\end{bmatrix}$<br/>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이런 방식으로 똑같이 $O1, O2$도 구할 수 있습니다.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Activation Function\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이전에 인공신경망을 설명드리면서 뉴런이 외부 뉴런으로 받은 신호들을 종합해서 다음 신경에게 신호를 보낼지 말지를 결정한다고 이야기했습니다.\n",
    "\n",
    "이를 인공신경망에서 구현하기 위해서 `Activation Function`을 사용하게됩니다.\n",
    "\n",
    "초기의 `Step Function`이라는 불연속적인 신호를 통해서 어떠한 임계값을 넘었을 때, 출력값을 보내고 임계점을 넘지 않았을 때는 \n",
    "\n",
    "출력값을 내보내지 않게하는 함수를 사용했습니다.\n",
    "\n",
    "`Step Function`은 `Figure 9`에 나타나있고, 수식은 다음과 같습니다.\n",
    "\n",
    "$$Step = \\begin{cases}\n",
    "    1 & \\mbox{if}& x \\in A, \\\\\n",
    "    0 & \\mbox {if}& x \\notin A. \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "<img src=\"images/step_fucntion.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center> <u>Figure 9</u>: Step Function</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "하지만 이런 `Step Function`은 나중에 인공 신경망을 학습 시킬 때, 사용할 수 없는 단점이 있습니다.\n",
    "\n",
    "나중에 더 자세하게 설명하겠지만, 이유를 간단히 말씀드리자면, 인공신경망을 학습할 때, 경사하강법을 통해서 학습하게 되는데,\n",
    "\n",
    "경사하강법은 미분을 사용합니다. 하지만 `Step Function`은 미분이 불가능한 함수이기 때문에, `Step Function`을 사용하게되면,\n",
    "\n",
    "경사하강법을 사용할 수가 없습니다.\n",
    "\n",
    "따라서 사람들은 경사하강법을 적용하기 위해서 `Activation Function`을 `Sigmoid Function`로 사용하게됩니다.\n",
    "\n",
    "`Step Function` 대신에 `Sigmoid Function`을 사용하는 이유는 `Sigmoid Function`이 `Step Function`과 비슷한 미분이 가능한 함수이기 때문입니다.\n",
    "\n",
    "`Sigmoid Function`의 그림은 `Figure 10`에 잘 나타나있으며, 수식은 다음과 같습니다.\n",
    "\n",
    "`Figure 10`에서 보여지는 것과 같이, `Sigmoid`함수의 계수에 따라서 `Step Function`과 유사한 형태를 띄는 것을 확인할 수 있습니다.\n",
    "\n",
    "$$Sigmoid = $$\n",
    "\n",
    "<img src=\"images/sigmoid_1.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center> <u>Figure 10</u>: Sigmoid Function</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 Cost Function\n",
    "<br/><br/>\n",
    "지금까지 MLP의 전파 알고리즘(Forward Operation)의 개념을 확인하였습니다. \n",
    "\n",
    "이렇게 MLP의 전파 알고리즘을 타고 최종적으로 출력된 값은 중요한 의미를 갖습니다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**예를 들어 어떠한 이미지를 입력데이터로 인공신경망에게 넣었을 때, 출력값은 이게 `개`인지 `고양이`인지에 대한 출력으로 볼 수 있습니다**.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "만약에 우리가 가중치값들을 임의로 준 상태에서 입력에 고양이 이미지를 넣는다면 \n",
    "\n",
    "출력값은 분명 정확히 이게 개인지 고양인지 출력값을 낼 수 없을 것입니다.\n",
    "<br/><br/>\n",
    "\n",
    "그렇기 때문에 우리는 인공 신경망에게 입력 데이터를 주면서 이 데이터는 **고양이**야 라고 알려주어야 하며, 인공 신경망은 이를 통해서\n",
    "\n",
    "내가 낸 출력값과 정답이 얼만큼 차이가 나는지를 계산해서 스스로 가중치의 값을 변화하는 과정을 거치게 됩니다.\n",
    "\n",
    "이러한 학습 방법을 딥러닝에서는 `지도 학습(Supervised Learning)`이라고 부릅니다.\n",
    "\n",
    "\n",
    "<img src=\"images/cat-dog-flow-horizontal.gif\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 11</u>: Cat and Dog Classifier with Neural Network</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "그러면 임의로 가중치의 값이 주어진 인공신경망이 입력데이터를 받아서 출력값을 냈을 때, 정답과 출력값이 다른지는 어떻게 알며,\n",
    "\n",
    "얼마나 틀렸는지에 대해서는 어떻게 알까요?\n",
    "\n",
    "이러한 부분에 대해서 중요한 역활을 하는 것이 `비용 함수(Cost Function)`이 됩니다.\n",
    "\n",
    "그럼 이번에는 인공 신경망에서 쓰이는 `비용 함수`에 대해서 본격적으로 이야기 해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\">Cross-Entropy Error** <br/><br/>\n",
    "\n",
    "최근 딮러닝에서 사용하는 비용 함수(Cost Function)는 Cross-Entropy Error를 사용합니다.\n",
    "\n",
    "Cross-Entory Error에 대한 수식은 다음과 같습니다.\n",
    "\n",
    "이해가 어려우시다면 뒤에(`비용이란?`) 설명이 나오니 수식이 이렇게 생겼다는 것만 확인하시고, 넘어가시면 됩니다.\n",
    "\n",
    "$$ J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\">비용이란?(What is Cost?)** <br/><br/>\n",
    "    \n",
    "만약에 인공 신경망이 다음과 같은 2가지 결과를 주었다고 가정해 보겠습니다.\n",
    "\n",
    "우리는 여기서 인공 신경망의 정확도가 몇이고, 얼마나 틀렸는지를 확인해야한다고 합시다\n",
    "\n",
    "그럴 때, 우리는 어떤 방법으로 이를 측정할 수 있을까요?\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "**1-th network**\n",
    "<table style=\"width:50%\", title=\"1-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.3  0.4 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "**2-th network**\n",
    "<table style=\"width:50%\", title=\"2-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.7  0.2 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "여기서 **분류 오차/MSE(Mean Squared Error)/CEE(Cross-Entroy Error)** 3가지 방법을 설명하고, 비용을 계산해서\n",
    "\n",
    "해당 방법들의 차이에 대해서 이야기해보도록 하겠습니다.\n",
    "\n",
    "자 이제, 위 결과에 대해서 3가지 방법(**분류 오차/MSE(Mean Squared Error)/CEE(Cross-Entroy Error)** 오차를 계산해보겠습니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "**1. 분류 오차**\n",
    "\n",
    "분류 오차는 인공 신경망의 출력값들이 정답과 비교했을 때, 몇개를 맞췄고 몇개를 못맞췄는지에 대해서 비교해서\n",
    "\n",
    "인공신경망의 정확도를 측정하는데 쓰입니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "- *1-th network*:\n",
    " - 분류오차 -> *(3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)*$$\\frac{1}{3} = 0.33$$ <br/><br/><br/> \n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:\n",
    " - 분류오차 -> *(3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)* $$\\frac{1}{3} = 0.33$$ <br/><br/><br/>\n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "\n",
    "두 오차에 대해서 비교해보면, **결과는 같지만**, 2-th 네트워크의 첫 두 샘플은 1-th 네트워크보다 조금 더 확실히 맞추었고, 세번째 샘플은 아깝게 틀렸다는 것을 확인할 수 있습니다.\n",
    "\n",
    "이러한 결과를 보았을 때, **단순 분류 오차**의 계산은 틀린 개수에 대한 결과만 줄 뿐 **정답과 비교해서 얼마나 많이 틀렸는지 얼마나 정확하게 맞았는지** 그 정도에 대한 값을 제공하지 않습니다\n",
    "<br/><br/><br/><br/>\n",
    "**2. MSE(Mean Squared Error)**\n",
    "<br/><br/><br/>\n",
    "`MSE(Mean Squared Error)`는 정답과 내가 얼마나 멀리 떨어져있는지에 대해서 정의하는 함수입니다.\n",
    "\n",
    "쉽게 이야기하기 위해서 내가 집에 가고 싶은데, 지금 위치에서 집까지의 거리를 확인해서 \n",
    "\n",
    "내가 집까지 가기 위한 거리값을 오차값으로 잡는 것을 `MSE`로 이해하면됩니다.\n",
    "\n",
    "MSE의 수식은 다음과 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "$$J = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\hat{Y_{i}}(\\hat{Y_{i}} - Y_{i})^{2}}$$\n",
    "\n",
    "<img src=\"images/scatter_plot.gif\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u>Figure 12</u>: Mean Squared Error</center></caption>\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:<br/><br/>\n",
    " - 분류오차 -> $0(0.3-0)^{2} + 0(0.3-0)^{2} + 1(0.4-1)^{2} = 0.36$ (나머지 2개의 샘플에 대해서는 생략..)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{(0.36 + 0.36 + 0.81)}{3} = 0.51$<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:<br/><br/>\n",
    " - 분류오차 -> (생략...)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{(0.09 + 0.09 + 0.49)}{3} = 0.22$<br/><br/>\n",
    "\n",
    "<br/><br/>\n",
    "**3. Cross-entropy Error**\n",
    "\n",
    "\n",
    "`CEE(Cross-Entropy Error)`는 내 출력값과 정답값이 얼마나 틀렸는지도 확인하고, 정답이 아닌 부분에 대해서도 얼마만큼\n",
    "\n",
    "차이가 있는지에 대해서 확인하는 함수입니다.\n",
    "\n",
    "<br/><br/>\n",
    "수식은 다음과 같습니다. \n",
    "\n",
    "<br/>\n",
    "$$J = -\\frac{1}{m}\\sum\\limits_{i = 1}^{m}(\\hat{Y_{i}}\\log\\left(Y_{i}\\right) + (1-\\hat{Y_{i}})\\log\\left(1- Y_{i}\\right))$$\n",
    "\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:<br/><br/>\n",
    "\n",
    " - 분류오차 -> (나머지 2개의 샘플에 대해서는 생략..)<br/><br/>$ -((0\\times\\log\\left(0.3\\right) + (1-0)\\times\\log\\left(1-0.3\\right)) + 0\\times\\log\\left(0.3\\right) + (1-0)\\times\\log\\left(1-0.3\\right) + 1\\times\\log\\left(0.4\\right) + (1-1)\\times\\log\\left(1-0.4\\right))$<br/><br/>\n",
    " = $(-0.356)+(-0.356)+(-0.916) = -1.628$<br/><br/>\n",
    " \n",
    " - 분류 정확도 -> <br/><br/>\n",
    " $\\frac{-(-2.472 + -3.723 + -1.628)}{3} = 2.607$<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:<br/><br/>\n",
    " - 분류오차 -> (생략...)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{-(-0.684 + -0.684 + -2.066)}{3} = 1.144$<br/><br/>\n",
    " \n",
    " \n",
    "<br/><br/>\n",
    "결과에서 볼 수 있듯이 MSE는 정답인 부분에 대해서 얼만큼 틀렸는지에 대해서만 집중합니다.\n",
    "\n",
    "따라서 `MSE`에서는 두가지 모델의 에러가 크게 차이가 없지만, `CEE`에서는 크게 차이가 나는 것을 확인할 수 있습니다.\n",
    "\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 경사하강법 (Gradient Descent Algorithm)\n",
    "<br/><br/>\n",
    "\n",
    "이번에는 인공 신경망에서 학습할 때, 쓰이는 경사하강법에 대해서 알아보겠습니다.\n",
    "\n",
    "경사하강법은 특정 함수에서 제일 작은 값을 찾아가는 알고리즘입니다.\n",
    "\n",
    "아까 위의 설명에서 비용함수에 대해서 이야기했었습니다.\n",
    "\n",
    "해당 비용함수의 수식은 에러에 대한 수식인데, 이러한 비용함수는 하나의 오차에 대한 3차원 표면으로 나타날 수 있습니다.\n",
    "\n",
    "이러한 3차원 표면을 에러 표면이라고 하며, 이러한 에러 표면은 `Figure 13`에 잘 나타나있습니다.\n",
    "\n",
    "경사하강법은 이러한 3차원 표면에서 제일 낮은 값. 즉, `Figure 13`에서 `Global Minima`라고 적힌\n",
    "\n",
    "제일 낮은곳으로 내려가게끔 만들어주는 알고리즘입니다.\n",
    "\n",
    "<img src=\"images/error_surface.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 13</u>: Error Surface </center></caption>\n",
    "\n",
    "\n",
    "경사하강법은 이러한 에러 표면에서 에러값이 제일 작은 곳을 찾아가는 알고리즘입니다. \n",
    "\n",
    "경사하강법의 직관적인 그림은 `Figure 14`에 잘 나타나있습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"images/sgd.gif\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 14</u>: Gradient Descent Algorithm </center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "경사하강법의 수식은 다음과 같습니다.\n",
    "\n",
    "**<font size=\"4\">$$x_{i+1} = x_{i} - \\gamma_{i}\\nabla{f(x_{i})}$$<br/>**\n",
    "    \n",
    "\n",
    "해당 수학 기호의 의미는 다음과 같습니다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "    \n",
    "$x_{i+1}$ : 다음 $x$의 값\n",
    "\n",
    "$x_{i}$ : 현재 $x$의 값\n",
    "\n",
    "$\\gamma_{i}$ : 학습율(learning rate) -> 학습율은 사람이 값을 임의로 설정해줘야하는 값입니다.\n",
    "\n",
    "\n",
    "$\\nabla{f(x_{i})}$ : $f(x_{i})$에서의 미분값 (현재 $x_{i}$의 함수값의 미분값) \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "그럼, 이제부터 `MSE` 비용 함수에 경사하강법 알고리즘을 적용해서 최소점을 찾아가는 방법을\n",
    "\n",
    "수식으로 풀어가면서 이야기하도록 하겠습니다.\n",
    "\n",
    "비용함수 챕터에서 `MSE` 수식을 언급했었습니다.\n",
    "\n",
    "**<font size=\"3\">$$J = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{(\\hat{Y_{i}} - Y_{i})^{2}}$$**\n",
    "\n",
    "여기서 $\\sum$와, $\\frac{1}{n}$를 빼고 수식 $(\\hat{Y_{i}} - Y_{i})^{2}$만 잘 보면, 결국에 정답 $\\hat{Y_{i}}$은 정해져있고, \n",
    "\n",
    "네트워크의 출력값 $Y_{i}^{2}$을 정답에 맞추면 비용함수가 값이 `0`이 된다는 것을 알 수 있습니다.\n",
    "\n",
    "위의 `MSE`함수를 우리 모두가 아는 일반적인 함수로 바꿔보겠습니다.\n",
    "\n",
    "$y = (x-c)^{2}$\n",
    "\n",
    "위의 함수 어디서 많이 본 거 같지 않나요? 바로 2차 함수에 대한 식입니다.<br/><br/>\n",
    "\n",
    "<img src=\"images/capture2-1.gif\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 15</u>: quadratic function </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "우리는 이러한 2차 함수 형태를 지니는 임의의 `MSE`함수를 정의하고, 경사하강법을 적용해서 `MSE`함수의 값을 0으로 수렴시켜보도록 하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이제 임의의 `MSE`함수를 아래와 같이 정의하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$y = x^{2}$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이러한 이차함수 `MSE`의 미분값은 아래와 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$y^{'} = 2x$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "이제 경사하강법을 시작하기 위해서 랜덤한 x값을 주고, 여기에서부터 시작해서 경사하강법을 진행해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이번 예제에서 경사하강법의 `학습률(learning rate)`은 `0.1`을 갖는다고 생각하고 진행하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "랜덤한 값 **$x$** 를 `3`으로 줬다고 가정합시다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "그럼 $y$값은 다음과 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$y=3^{2} = 9$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "그럼 이제 여기에서 경사하강법을 적용해봅시다.\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "경사하강법의 수식은 위에서 이야기했듯이 아래와 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\">$$x_{i+1} = x_{i} - \\gamma_{i}\\nabla{f(x_{i})}$$**\n",
    "<br/><br/>\n",
    "위의 수식을 그대로 따라서 적용해보면 다음과 같습니다.\n",
    "<br/><br/>\n",
    "\n",
    "- 1번째 스텝<br/><br/>\n",
    "$x_{1} = 3 - 0.1 \\times y^{'} = 3 - 0.1 \\times 2x = 3 - 0.1 \\times 6 = 3-0.6 = 2.4$<br/><br/>\n",
    "\n",
    "- 2번째 스텝<br/><br/>\n",
    "$x_{2} = 2.4 - 0.1 \\times y^{'} = 2.4 - 0.1 \\times 2x = 2.4 - 0.1 \\times 4.8 = 2.4-0.48 = 1.92$<br/><br/>\n",
    "\n",
    "- 3번째 스텝<br/><br/>\n",
    "$x_{3} = 1.92 - 0.1 \\times y^{'} = 1.92 - 0.1 \\times 2x = 1.92 - 0.1 \\times 3.84 = 1.92-0.384 = 1.536$<br/><br/>\n",
    "\n",
    "\n",
    "확인해보시면 결과가 계속해서 줄어는 것을 확인할 수 있습니다.\n",
    "\n",
    "이렇게 계속 스텝을 진행하다보면 함수의 값이 점점 `0`에 가까워지는 것을 확인하실 수 있습니다.<br/>\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/><br/><br/><br/>\n",
    "    \n",
    "- 학습할 때, `학습률(learing rate)`의 값은 항상 신경써서 맞춰줘야합니다. `학습률`을 너무 낮게 주면, 학습하는데 큰 시간이 걸리고, `학습률`을 너무 크게주면, 학습이 안될 수 있습니다. 아래 Figure 16은 `학습률(learning rate)`을 잘못 주었을 때, 어떤 현상이 나타나는지 보여주는 그림입니다.\n",
    "<br/><br/>\n",
    "<img src=\"images/sgd_bad.gif\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 14</u>: Bad Learning rate </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5 역 전파 알고리즘(Backpropagation)\n",
    "\n",
    "<br/>\n",
    "\n",
    "그럼 위에 이야기한 경사하강법을 어떻게 인공 신경망에 적용하는지에 대해서 알아보겠습니다.\n",
    "\n",
    "다시 한번 간단한 인공신경망의 구조를 확인하고 넘어가도록 하겠습니다.\n",
    "\n",
    "<img src=\"images/neural_network-7.png\" style=\"width:500px;height:400px;\">\n",
    "<caption><center> <u>Figure 15</u>: Artificial Neural Network </center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "<br/>\n",
    "역 전파 알고리즘(Backpropagation)은 `체인 룰(Chain rule)` 이라는 것에 기반을 두고 있습니다.\n",
    "\n",
    "`체인 룰(Chain rule)`하면 뭔가 딱 떠오르는게 없나요? 우리 많이 하는 게임 보면 `체인 라이트닝`이라는 스킬을 많이 볼 수 있습니다.\n",
    "\n",
    "`체인 룰(Chain rule)`은 바로 이 `체인 라이트닝`과 같은 역활을 합니다.\n",
    "\n",
    "첫 한방만 맞으면 연쇄반응을 일으키면서 뒤로 전파되는 것이죠.\n",
    "<br/><br/><br/>\n",
    "<img src=\"images/chain_lightning.jpg\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u>Figure 16</u>: Chain Lightning </center></caption>\n",
    "<br/><br/>\n",
    "딥러닝에서는 학습하는 역 전파 알고리즘을 이러한 `체인 룰(Chain rule)` 통해서 학습하게 됩니다.\n",
    "\n",
    "무수히 연결된 신경들에게 `데이터가 입력된 방향`으로 신경 노드들 개별의 `미분값`을 가지고있으면 \n",
    "\n",
    "이들을 `인공 신경망의 출력`에서 `데이터가 입력된 방향`으로 개별의 신경 노드들의 `미분값`들을 곱해가면서\n",
    "\n",
    "위에서 설명한 경사하강법(Gradient Descent)를 적용해서 `파라미터`들을 업데이트해주는 것을 **`역 전파 알고리즘(Backpropagation)`** 이라고 합니다.\n",
    "\n",
    "결국에 `Figure 16`과 같이 `소서리스`의 `체인 라이트닝`과 같은 스킬을 딥러닝이 쓰게 되는 것입니다.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "<img src=\"images/output_1_backprop-4.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> <u>Figure 17</u>: Back Propagation </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "<img src=\"images/neural_network-7.png\" style=\"width:700px;height:600px;\">\n",
    "<caption><center> <u>Figure 18</u>: Artificial Neural Network </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "결국 여기서 우리가 경사하강법 알고리즘을 통해서 업데이트하고싶은 값은 가중치(Weights)값입니다.\n",
    "\n",
    "Figure 18에서 가중치 값은 다음과 같은 행렬값입니다.\n",
    "\n",
    "<br/>\n",
    "$W^{1} = \\begin{bmatrix}\n",
    "    W^{1}_{11}  & W^{1}_{21}\\\\\n",
    "    W^{1}_{12}  & W^{1}_{22}\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "$W^{2} = \\begin{bmatrix}\n",
    "    W^{2}_{11}  & W^{2}_{21}\\\\\n",
    "    W^{2}_{12}  & W^{2}_{22}\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/><br/>\n",
    "  \n",
    "<font size=\"3\">$W^{a}_{bc}$<br/>\n",
    "\n",
    "- $a$ : 인공 신경망에서 레이어 순서를 의미\n",
    "- $bc$ : 인공 신경망에서 b번째 노드에서 c번째 노드로 향함을 의미\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**ex>** <br/>\n",
    "\n",
    "- $W^{1}_{11}$ : 첫번째 레이어의 신경망에서 1번째(상단)노드에서 다음 레이어의 1번째 노드(상단)로 향하는 가중치를 의미합니다\n",
    "    \n",
    "\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/backpropagation.png\" style=\"width:700;height:600px;\">\n",
    "<caption><center> <u>Figure 19</u>: Back Propagation </center></caption>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "이제 그럼 이를 직접 풀어보도록 하겠습니다. 여기서는 문제를 조금 쉽게 풀기위해서 `비용 함수(Cost Function)`은 `MSE`로 하고,\n",
    "`learning rate`는 `0.5`로 하겠습니다. $O_{1}$의 정답은 `0.01`이고, $O_{2}$의 정답은 `0.99`로 가정합니다.\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/neural_network-9.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> <u>Figure 20</u>: Artificial Neural Network </center></caption>\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "$I = \\begin{bmatrix} 0.05 & 0.1 \\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$W^{1} = \\begin{bmatrix} 0.15 & 0.25\\\\0.20 & 0.30 \\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$b^{1} = \\begin{bmatrix} 0.35\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$W^{2} = \\begin{bmatrix} 0.40 & 0.50\\\\0.45 & 0.55 \\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$b^{2} = \\begin{bmatrix} 0.60\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "**<font size=\"4\"> 1. 전파 알고리즘** \n",
    "    \n",
    "이제 가중치들을 이용해서 전파 알고리즘을 수행합니다.\n",
    "\n",
    "전파 알고리즘을 수행해서 우리는 $O_{1}$과 $O_{2}$의 값을 구할 수 있습니다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**1-1. $Z^{1}$** = $IW^{1} + b^{1} = \\begin{bmatrix} 0.05 & 0.1 \\end{bmatrix} \\times \\begin{bmatrix} 0.15 & 0.25\\\\0.20 & 0.30 \\end{bmatrix} + \\begin{bmatrix} 0.35\\end{bmatrix} = \\begin{bmatrix} 0.0275 & 0.0425 \\end{bmatrix} + \\begin{bmatrix} 0.35\\end{bmatrix} = \\begin{bmatrix} 0.3755 & 0.3925 \\end{bmatrix} $\n",
    "\n",
    "**1-2. $A^{1}$** = $Sigmoid(Z^{1}) = \\begin{bmatrix} Sigmoid(0.3755) & Sigmoid(0.3925) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{1+e^{-0.3755}} & \\frac{1}{1+e^{-0.3925}}\\end{bmatrix} = \\begin{bmatrix} 0.593 & 0.597 \\end{bmatrix}$\n",
    "\n",
    "**1-3. $Z^{2}$** = $A^{1}W^{2} + b^{2} = \\begin{bmatrix} 0.593 & 0.597 \\end{bmatrix} \\times \\begin{bmatrix} 0.40 & 0.50\\\\0.45 & 0.55 \\end{bmatrix} + \\begin{bmatrix} 0.60\\end{bmatrix}= \\begin{bmatrix} 1.106 & 1.224 \\end{bmatrix}$\n",
    "\n",
    "**1-4. $A^{2}$** = $Sigmoid(Z^{2}) = \\begin{bmatrix} Sigmoid(1.106) & Sigmoid(1.224) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{1+e^{-1.106}} & \\frac{1}{1+e^{-1.224}}\\end{bmatrix} = \\begin{bmatrix} 0.751 & 0.772 \\end{bmatrix}$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\"> 2. 비용 측정** \n",
    "    \n",
    "전파 알고리즘을 통해 나온 결과값과 실제 정답셋과의 차이(비용)을 `MSE`함수를 통해서 측정합니다.\n",
    "\n",
    "$$J = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{(\\hat{Y_{i}} - Y_{i})^{2}}$$\n",
    "<br/><br/>\n",
    "$J_{1} = (\\hat{Y_{i}} - Y_{i})^{2} = (0.01 - 0.751)^{2} = 0.549081$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$J_{2} = (\\hat{Y_{i}} - Y_{i})^{2} = (0.99 - 0.772)^{2} = 0.047524$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$J_{total} =  \\frac{1}{2}(J_{1} + J_{2})= \\frac{1}{2}(0.549081 + 0.047524) = 0.596605$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\"> 3. 역 전파 알고리즘** \n",
    "    \n",
    "<br/>\n",
    "\n",
    "**<font size=\"3\"> 3-1. 체인 룰을 적용하기 위한 미분값 구하기** \n",
    "\n",
    "<br/>\n",
    "\n",
    "이제 Figure 21과 같이 각 노드들의 미분값들을 모두 구해보도록 하겠습니다.\n",
    "\n",
    "시작하기전에 아래와 같은 수식이 나올 것 입니다.\n",
    "\n",
    "$$\\frac{\\partial A}{\\partial B}$$\n",
    "\n",
    "해당 수식은 `편미분`이라고 불리는 것입니다.\n",
    "\n",
    "위의 `편미분`식이 의미하는 것은 `A`에 `B`가 기여한 정도를 의미합니다.\n",
    "\n",
    "앞으로 나올 미분하는 방법에 대해서는 미분을 잘 모르신다면, 어떻게 미분이 되는지 자세히 모르셔도 됩니다.\n",
    "그냥 이 식을 미분하면 이렇게 미분이 된다는 것만 아시면 됩니다.\n",
    "\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/backpropagation.png\" style=\"width:700;height:600px;\">\n",
    "<caption><center> <u>Figure 21</u>: Back Propagation </center></caption>\n",
    "<br/>\n",
    "\n",
    "$J = \\frac{1}{2}\\sum\\limits_{i=1}^{2}{(\\hat{Y_{i}} - Y_{i})^{2}}$<br/>\n",
    "$J = \\frac{1}{2}((\\hat{Y_{1}} - Y_{1})^{2}) + (\\hat{Y_{2}} - Y_{2})^{2})$\n",
    "<br/>\n",
    "- $\\frac{\\partial J}{\\partial O_{1}} = 2 * \\frac{1}{2}(\\hat{Y_{1}} - O_{1})^{2-1} \\times -1 + 0 = -(\\hat{Y_{1}} - O_{1})$\n",
    "<br/>\n",
    "- $\\frac{\\partial J}{\\partial O_{2}} = 2 * \\frac{1}{2}(\\hat{Y_{2}} - O_{2})^{2-1} \\times -1 + 0 = -(\\hat{Y_{2}} - O_{2})$\n",
    "\n",
    "<br/><br/>\n",
    "$Sigmoid = \\frac{1}{1+e^{-x}}$<br/>\n",
    "$Sigmoid^{'} = Sigmoid(1-Sigmoid) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}})$\n",
    "<br/>\n",
    "- $\\frac{\\partial O_{1}}{\\partial Z^{2}_{1}} =  O_{1}(1-O_{1})$\n",
    "<br/>\n",
    "- $\\frac{\\partial O_{2}}{\\partial Z^{2}_{2}} =  O_{2}(1-O_{2})$\n",
    "\n",
    "<br/><br/>\n",
    "$Z = WX + b$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{2}}{\\partial W_{11}^{2}} = \\frac{\\partial(W_{11}^{2}A_{1}^{1} + W_{21}^{2}A_{1}^{2} + b^{2})}{\\partial W_{11}^{2}} = A_{1}^{1}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{2}}{\\partial W_{21}^{2}} = \\frac{\\partial(W_{11}^{2}A_{1}^{1} + W_{21}^{2}A_{1}^{2} + b^{2})}{\\partial W_{21}^{2}} = A_{1}^{2}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{2}^{2}}{\\partial W_{12}^{2}} = \\frac{\\partial(W_{12}^{2}A_{1}^{1} + W_{22}^{2}A_{1}^{2} + b^{2})}{\\partial W_{12}^{2}} = A_{1}^{1}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{2}^{2}}{\\partial W_{22}^{2}} = \\frac{\\partial(W_{12}^{2}A_{1}^{1} + W_{22}^{2}A_{1}^{2} + b^{2})}{\\partial W_{22}^{2}} = A_{1}^{2}$\n",
    "\n",
    "<br/><br/>\n",
    "$Sigmoid = \\frac{1}{1+e^{-x}}$<br/>\n",
    "$Sigmoid^{'} = Sigmoid(1-Sigmoid) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}})$\n",
    "<br/>\n",
    "- $\\frac{\\partial A_{1}^{1}}{\\partial Z^{1}_{1}} =  A_{1}(1-A_{1})$\n",
    "<br/>\n",
    "- $\\frac{\\partial A_{2}^{1}}{\\partial Z^{1}_{2}} =  A_{2}(1-A_{2})$\n",
    "\n",
    "<br/><br/>\n",
    "$Z = WX + b$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}} = \\frac{\\partial(W_{11}^{1}I_{1} + W_{21}^{1}I_{2} + b^{1})}{\\partial W_{11}^{1}} = I_{1}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{1}}{\\partial W_{21}^{1}} = \\frac{\\partial(W_{11}^{1}I_{1} + W_{21}^{1}I_{2} + b^{1})}{\\partial W_{21}^{1}} = I_{2}$\n",
    "<br/>\n",
    "-  $\\frac{\\partial Z_{2}^{1}}{\\partial W_{12}^{1}} = \\frac{\\partial(W_{12}^{1}I_{1} + W_{22}^{1}I_{2} + b^{1})}{\\partial W_{12}^{1}} = I_{1}$\n",
    "<br/>\n",
    "-  $\\frac{\\partial Z_{2}^{1}}{\\partial W_{22}^{1}} = \\frac{\\partial(W_{12}^{1}I_{1} + W_{22}^{1}I_{2} + b^{1})}{\\partial W_{22}^{1}} = I_{2}$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "이제 모든 미분값들을 구했으니, 이 미분값들을 이용해서 경사하강법 알고리즘을 통해서 가중치값을 업데이트하는 과정을 보겠습니다.\n",
    "\n",
    "예를 들어서 우리가 $W_{11}^{2}$의 가중치 값을 업데이트하고 싶다면, 인공신경망 출력부분에서 \n",
    "\n",
    "$W_{11}^{2}$까지 오는 `체인룰(Chain rule)`이라는 개념을 사용해서 모두 곱해주면 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial W_{11}^{2}} = \\frac{\\partial Z_{2}^{1}}{\\partial W_{11}^{2}} \\times \\frac{\\partial O_{1}}{\\partial Z_{2}^{1}} \\times \\frac{\\partial E}{\\partial O_{1}}$\n",
    "\n",
    "해당 수식을 아까 미분값을 구한 것으로 바꾸면 아래와 같이 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial W_{11}^{1}} = A^{1}_{1} \\times O_{1}(1-O_{1}) \\times (\\hat{Y_{1}} - O_{1})$\n",
    "\n",
    "여기서 $A^{1}_{1}$, $O_{1}$, $\\hat{Y_{1}}$등과 같은 숫자들은 이미 전파 알고리즘에서 우리가 다 구한 값입니다.\n",
    "\n",
    "따라서 그 결과들을 그래로 넣어서 계산을 하면 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial W_{11}^{2}} = 0.593 \\times 0.187 \\times 0.741 = 0.082170231$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "자 이렇게 우리는 전체 비용(에러)에 대한 $W_{11}^{2}$의 기여도를 구했습니다.\n",
    "\n",
    "그럼 이제 경사하강 알고리즘을 통해서 $W_{11}^{2}$값을 업데이트 해보겠습니다.\n",
    "\n",
    "경사하강 알고리즘은 다음과 같은 수식을 따른다고 말했었습니다.\n",
    "\n",
    "<br/>\n",
    "**<font size=\"4\">$$x_{i+1} = x_{i} - \\gamma_{i}\\nabla{f(x_{i})}$$**\n",
    "<br/>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "그리고 우리는 학습률(learning rate)의 값 $\\gamma_{i}$을 `0.5`로 하기로 했었습니다.\n",
    "\n",
    "$\\nabla{f(x_{i})}$는 `미분값`이므로 전체 비용에 대한 $x$에 대한 기여도입니다.\n",
    "\n",
    "자 이제 우리가 여태까지 구했던 값들을 이용해서 새로운 $W_{11}^{1}$값을 구해봅시다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "$(W_{11}^{2})^{+} = (W_{11}^{2}) - 0.5 \\times \\frac{\\partial E}{\\partial W_{11}^{1}} = 0.40 - (0.5 \\times 0.082170231) = 0.358914885$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이렇게 우리는 2번째 레이어의 가중치값들을 다음과 같이 모두 업데이트할 수 있습니다.\n",
    "\n",
    "$(W_{21}^{2})^{+} = 0.408666186$\n",
    "\n",
    "\n",
    "$(W_{12}^{2})^{+} = 0.511301270$\n",
    "\n",
    "\n",
    "$(W_{22}^{2})^{+} = 0.561370121$\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "그럼 이제 1번째 레이어의 가중치들도 업데이트를 해보겠습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{11}^{1}} = \\frac{\\partial E}{\\partial A_{1}^{1}} \\times \\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} \\times \\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "여기서 위에서 구하지 않았던 새로운 수식 $\\frac{\\partial E}{\\partial A_{1}^{1}}$ 을 확인할 수 있습니다.\n",
    "\n",
    "해당 수식이 의미하는 바는 아래의 수식과 같습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial A_{1}^{1}} = \\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}} + \\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}}$$\n",
    "\n",
    "여기서 $\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}}$는 다음과 같습니다.\n",
    "아래의 숫자는 이미 위에서 모두 구해놨던 값을 참조하였습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}} = \\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}}\\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{2}} = 0.741 \\times 0.186 = 0.138$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "즉 신경노드 `h1의 출력값`이 전체 비용(에러)에 얼마만큼 기여했는지에 대한 내용입니다.\n",
    "\n",
    "우리가 2번째 레이어의 가중치를 업데이트할 때, 계산했었던 값들을 아래의 식에 대입하면 다음과 같습니다.\n",
    "\n",
    "$\\frac{\\partial E}{\\partial O_{1}}\\frac{\\partial O_{1}}{\\partial Z_{1}^{2}} = 0.741 \\times 0.186 = 0.138$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$\\frac{\\partial Z_{1}^{2}}{\\partial A_{1}^{1}}$은 $W_{12}^{2}$와 같습니다.\n",
    "\n",
    "$\\frac{\\partial Z_{1}^{2}}{\\partial A_{1}^{1}} = W_{12}^{2} = 0.40$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이를 조합하면 다음과 같습니다.\n",
    "\n",
    "$\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{2}} = \\frac{\\partial E_{O_{1}}}{\\partial Z_{1}^{2}}\\frac{\\partial Z_{1}^{2}}{\\partial A_{1}^{2}} = 0.138498562 * 0.40 = 0.055399425$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "위와 같이 $\\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}}$를 구하게되면, 아래와 같습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}} = -0.019049119$$\n",
    "\n",
    "따라서 다음과 같은 결과를 얻을 수 있습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial A_{1}^{1}} =\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}} + \\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}} = 0.055399425 + -0.019049119 = 0.036350306$$\n",
    "\n",
    "이제, 아래에서 $\\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}}$와 $\\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}}$를 구해보도록 하겠습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial A_{1}^{1}} \\times \\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} \\times \\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}}$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} = A_{1}^{1}(1-A_{1}^{1}) = 0.59326999(1-59326999) = 0.241300709$\n",
    "\n",
    "\n",
    "$\\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}} = I_{1} = 0.05$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "모두 종합해보면 아래와 같습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{11}^{1}} = \\frac{\\partial E}{\\partial A_{1}^{1}} \\times \\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} \\times \\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}} = 0.036350306 \\times 0.241300709 \\times 0.05 = 0.000438568$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "마지막으로 경사하강법을 적용해 가중치값을 업데이트하면 다음과 같습니다.\n",
    "\n",
    "$(W_{11}^{1})^{+} = (W_{11}^{1}) - 0.5 \\times \\frac{\\partial E}{\\partial W_{11}^{1}} = 0.15 - (0.5 \\times 0.000438568) = 0.149780716$\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "이와 같은 방법으로 1번째 레이어의 가중치들을 업데이트하게되면, 다음과 같습니다.\n",
    "\n",
    "$(W_{12}^{1})^{+} = 0.24975114$\n",
    "\n",
    "$(W_{21}^{1})^{+} = 0.19956143$\n",
    "\n",
    "$(W_{22}^{1})^{+} = 0.29950229$\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "자! 이제 전반적인 MLP에 사용되는 알고리즘을 모두 살펴보았습니다.\n",
    "\n",
    "다음 스크립트에서 Python의 numpy를 이용해서 위에서 설명한 알고리즘들을 모두 코드로 구현해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/><br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
