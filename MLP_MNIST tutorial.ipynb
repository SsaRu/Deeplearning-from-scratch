{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<br/>\n",
    "\n",
    "해당 스크립트에서는 Multi Layer Perceptron이라고 불리는 기본적인 인공 신경망을 Python 라이브러리인 numpy를 이용하여 구현한 스크립트입니다.\n",
    "\n",
    "해당 스크립트는 다음과 같으신 분을 위해서 쓰여졌습니다.\n",
    "\n",
    "1. Python 기초를 아시는 분\n",
    "2. 기본적인 딥러닝의 이론에 대해서 이해가 있으신분. \n",
    "3. Tensorflow나 Torch등 기본적인 프레임워크로도 구현을 해봤는데, 내부 동작은 어떻게 돌아가는지에 대해서 궁금하신 분\n",
    "\n",
    "<br/>\n",
    "만약에 해당 스크립트가 너무 어렵게 느껴지시거나 이론을 조금 더 알고 싶으시다면 아래의 링크에서 딥러닝의 기초에 대해서 배울 수 있으니, 먼저 기본적인 딥러닝에 대해서 학습하고 오시기를 권장해드립니다\n",
    "\n",
    "<br/>\n",
    "[<font size=\"3\">[1]. 모두의 딥러닝 (무료/국문)](https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)\n",
    "<br/><br/>\n",
    "[<font size=\"3\">[2]. 헬로 딥러닝 (무료/국문)](https://www.youtube.com/watch?v=yWySw4EfSJc&t=424s)\n",
    "<br/><br/>\n",
    "[<font size=\"3\">[3]. 코세라, Deep Learning Specialization (유료/영문)](https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&campaignid=917423980&adgroupid=46295378339&device=c&keyword=coursera%20deep%20learning&matchtype=p&network=g&devicemodel=&adpostion=1t1&creativeid=217989182387&hide_mobile_promo&gclid=EAIaIQobChMI_cnop7HU1wIVzgMqCh2CdAQiEAAYASAAEgKbcfD_BwE)\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Neural Network :: Multi Layer Perceptron\n",
    "\n",
    "\n",
    "<br/>\n",
    "시작하기 앞서서 해당 스크립트의 핵심은 다음과 같이 구현되어 있습니다.\n",
    "\n",
    "- 기본적인 인공신경망(Multi Layer Perceptron)에 대해서 설명합니다.\n",
    "\n",
    "\n",
    "- <전파 알고리즘(Forward Operation)> / 역전파 알고리즘(BackPropagation) / 활성화함수(Activation function) / 분류기(Softamx) / 에러함수(MSE, Cross-Entropy Error) 에 대한 작은 모듈들을 만듭니다.\n",
    "\n",
    "\n",
    "- 해당 모듈을 만들고나면, 이를 연결하여 하나의 인공신경망을 만듭니다.\n",
    "\n",
    "\n",
    "- 이에 대한 결과를 확인합니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "**<font size=\"5\">목차**\n",
    "\n",
    "**<font size=\"3\">1. MNIST DataSet 설명 및 데이터 로드**<br/>\n",
    "\n",
    "**<font size=\"3\">2. Multi-Layer Perceptron Overview**\n",
    "    \n",
    "**<font size=\"3\">3. 전파 알고리즘(Forward Operation) 개념 :: 기초**\n",
    "    \n",
    "**<font size=\"3\">4. 전파 알고리즘(Forward Operation) 개념 :: 행렬**\n",
    "\n",
    "**<font size=\"3\">5. 활성화 함수(Activation Function) 개념**\n",
    "\n",
    "**<font size=\"3\">6. 비용 함수(Cost Function) 개념 :: 분류 오차/MSE/CEE**\n",
    "\n",
    "**<font size=\"3\">7. 경사 하강 알고리즘(Gradient Descent) 개념 **\n",
    "\n",
    "**<font size=\"3\">8. 역 전파 알고리즘(Backpropagation) 개념 **\n",
    "    \n",
    "**<font size=\"3\">9. 가중치(weights) 생성 및 초기화 :: 단일 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">10. 가중치(weights) 생성 및 초기화 :: 다중 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">11. 전파 알고리즘(Forward Operation) :: 단일 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">12. 활성화 함수(Activation Function)와 활성화 함수의 미분 (Code)**\n",
    "\n",
    "**<font size=\"3\">13. 다중 전파 알고리즘(Forward Operation) :: 다중 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">14. 비용 함수(Cost Function) (Code)**\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">15. TODO 역 전파 알고리즘(Backpropagation) (Code)**\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">16. TODO MNIST 학습 :: numpy 기반 (Code)**\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">17. TODO MNIST 학습 :: tensorflow 기반 (Code)**\n",
    "\n",
    "<br/>\n",
    "<br/>*<font color=\"red\">해당 스크립트는 코세라의 Deep Learning Specialization의 코드를 기반으로 했음을 알려드립니다*\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - MNIST Data Load \n",
    "\n",
    "먼저, 예제 데이터를 입력으로 사용하기 위해서 MNIST 데이터를 로드하겠습니다.\n",
    "\n",
    "MNIST 데이터는 Figure 1과 같이 사람들의 다양한 손글씨 이미지를 대량으로 모아놓은 데이터입니다.\n",
    "<br/><br/>\n",
    "<img src=\"images/14481-mnist-dataset.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 1</u>: MNIST DataSet </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "일반적으로 모든 이미지는 데이터로 변환할 수 있는데, 아래의 Figure 2와 같이 하얀색에 가까울 수록 값이 0에 가까워지고\n",
    "검은색에 가까워질 수록 값이 1에 가까워지는 형태로 표현할 수 있습니다.\n",
    "<br/><br/>\n",
    "그림과 같이 MNIST 데이터는 다양한 손글씨 데이터를 width=28, height=28의 흑백이미지(검은색과 하얀색으로 이루어진)의 데이터 집합이라고 보시면 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "*<font color=\"red\">이해를 위해서 그림과 일치하게 설명한 것일 뿐이므로, 컨셉적으로만 이해하시면 됩니다.*<br/><br/>\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/d4e5709ebb4ba940126de44c76ca71b0.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 2</u>: Image Data </center></caption>\n",
    "\n",
    "\n",
    "MNIST 데이터의 특징은 다음과 같습니다.\n",
    "\n",
    "- 이미지의 width와 height가 모두 28입니다.\n",
    "\n",
    "\n",
    "- 흑백 이미지입니다.\n",
    "\n",
    "\n",
    "- 따라서 MNIST 데이터의 갯수는 $28x28x1=784$이 됩니다.<br/>\n",
    "*<font color=\"red\">이 또한 이해를 위해서 간략하게 설명한 것입니다. 지금은 이해하는 것이 중요하므로 그냥 그렇구나 넘어가시면 됩니다.*<br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - 1 MNIST Data Load \n",
    "\n",
    "<br/>\n",
    "**Note**: 일반적으로 이미지 데이터를 읽어들이게되면 numpy의 dtype(\"uint8\")으로 저장이 됩니다. uint8은 0~255 사이의 값으로 표현되며, 이를 \"0\"과 \"1\"사이의 데이터로 정규화하기 위해서 255로 나누어주는 과정을 거칩니다.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import bigfloat\n",
    "bigfloat.exp(5000,bigfloat.precision(100))\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Data Load\n",
    "def read_data(label_path, image_path):\n",
    "    with gzip.open(label_path) as flbl:\n",
    "        _, _ = struct.unpack(\">II\", flbl.read(8))\n",
    "        label = np.fromstring(flbl.read(), dtype=np.int8)\n",
    "    with gzip.open(image_path, 'rb') as fimg:\n",
    "        _, _, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n",
    "    return (label, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "(train_label, train_img) = read_data(path+'train-labels-idx1-ubyte.gz', path+'train-images-idx3-ubyte.gz')\n",
    "(val_label, val_img) = read_data(path+'t10k-labels-idx1-ubyte.gz', path+'t10k-images-idx3-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAACBCAYAAABXearSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCQQAAAAAAOCB/wPacveO5Y1kGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa8a8f3c6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(train_img[i], cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "train_label = train_label.astype(np.int64)\n",
    "train_img = train_img.astype(np.float32) / 255\n",
    "val_label = val_label.astype(np.int64)\n",
    "val_img = val_img.astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## 2 - Multi Layer Perceptron :: Overview\n",
    "\n",
    "<br/><br/>\n",
    "생물학에서 뉴런은 Figure 3의 왼쪽 하단 같이 생겼습니다.\n",
    "\n",
    "생물학적 뉴런은 다음과 같은 행동을 합니다.\n",
    "\n",
    "- 다른 뉴런들로부터 신호를 받는다.\n",
    "- 그 신호들을 종합한다.\n",
    "- 그 종합한 결과를 통해서 다음 뉴런에게 신호를 보낼지 말지 결정한다.\n",
    "<br/><br/>\n",
    "\n",
    "이러한 행동을 모델링한 것이 인공 신경망입니다.인공 신경도 똑같은 행동을 합니다.\n",
    "<br/>\n",
    "\n",
    "- 다른 뉴런들로부터 신호를 받는다.\n",
    "- 그 신호들을 종합한다.\n",
    "- 그 종합한 결과를 통해서 다음 뉴런에게 신호를 보낼지 말지 결정한다.\n",
    "\n",
    "이것이 기본적인 인공 신경망의 컨셉입니다.\n",
    "\n",
    "이러한 기본적인 컨셉을 가지고 여러가지 인공 신경을 연결해 놓은 것을 인공 신경망이라고 합니다.\n",
    "\n",
    "이를 다른 말로는 MLP(Multi Layer Perceptron)이라고 합니다.\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"images/neuron.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> <u>Figure 3</u>: Biological Neuron & Artificial Neuron</center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "이제부터 본격적으로 인공신경망에 대해서 자세하게 살펴보겠습니다. \n",
    "<br/>\n",
    "MLP(Multi Layer Perceptron :: 이제부터 편의상 MLP라고 줄여서 이야기하도록 하겠습니다)불리는 인공신경망의 계산 순서는 다음과 같습니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "1.실행 :: 학습이 완료되었을 경우<br/><br/>\n",
    "\n",
    "학습이 완료되었음을 가정했을 때, 인공신경망의 작동순서는 아래와 같이, 데이터를 입력하고,\n",
    "\n",
    "입력된 데이터는 전파 알고리즘을 통해서 인공신경망을 통과하게되고, 그 결과를 그대로 사용하게됩니다.\n",
    "\n",
    "- 데이터 입력 \n",
    "- 전파 알고리즘 (Forward Operation)\n",
    "<br/>\n",
    "\n",
    "<img src=\"images/MLP_test.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> <u>Figure 4</u>: MLP Test</center></caption>\n",
    "\n",
    "\n",
    "2.학습 \n",
    "\n",
    "만약 인공신경망이 학습이 덜 되었다면, 인공신경망을 학습시키는 과정을 거쳐야합니다.\n",
    "\n",
    "학습과정은 아래와 같이 데이터를 입력하고, 전파 알고리즘을 통해서 얻은 결과와 정답사이의 오차를 측정해서\n",
    "\n",
    "해당 오차를 역전파 알고리즘으로 전파하여 가중치들의 값을 업데이트해주게 됩니다.\n",
    "\n",
    "위의 과정을 학습이 완료될 때 까지 반복해서 학습하게 됩니다.\n",
    "\n",
    "\n",
    "- 데이터 입력\n",
    "- 전파 알고리즘 (Forward Operation)\n",
    "- 비용 함수 계산 (Cost Calculation)\n",
    "- 역전파 알고리즘 (Backpropagation)\n",
    "- 위의 4가지 순서를 반복 (Iteration)\n",
    "\n",
    "<img src=\"images/MLP_train.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> <u>Figure 5</u>: MLP Train</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "Figure 6은 그림에서 위의 알고리즘이 어디에 해당하는지 표시한 그림입니다.\n",
    "\n",
    "<img src=\"images/MLP_train_detail.png\" style=\"width:850px;height:400px;\">\n",
    "<caption><center> <u>Figure 6</u>: MLP Train Detail</center></caption>\n",
    "\n",
    "### 2-1 전파 알고리즘(Forward Operation) :: 기본 컨셉 (basic concept)\n",
    "\n",
    "<br/>\n",
    "\n",
    "앞에서 이야기했듯이 생물학적 신경망을 수학적으로 모델링한 것이 인공 신경망이라고 말씀 드렸습니다.\n",
    "\n",
    "그럼 이번 챕터에서는 다른 신경들로부터 받은 신호들을 어떻게 종합해서 결과를 출력하는지에 대해서 설명드리도록 하겠습니다.\n",
    "\n",
    "다른 신경들로부터 받은 신호를 종합하는 과정을 인공신경망에선느 전파 알고리즘(Forward Operation)이라고 합니다.\n",
    "\n",
    "이러한 전파 과정들에 대해서 나타낸 그림이 바로 Figure 6입니다\n",
    "\n",
    "Figure 7에 나타난 각 숫자들과 문자들에 대해서는 그림 아래에 Notation이 있습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"images/Multilayer-Perceptron-Forward-Propagation.png\" style=\"width:650px;height:550px;\">\n",
    "<caption><center> <u>Figure 7</u>: 1-Layer MLP </center></caption>\n",
    "\n",
    "\n",
    "**<font size=\"3\"><font color=\"red\">Note**: <br/><br/>\n",
    "*$l1,l2$* : 각각 하나의 입력데이터를 의미합니다.\n",
    "\n",
    "*화살표의 숫자* : 가중치 값을 의미합니다. 해당 스크립트에서는 $l1$에서 $H1$으로 가는 가중치를 $W_{l_{1}}^{H_{1}}$이라고 하겠습니다\n",
    "\n",
    "**$H1, H2, H3$** : 각각의 뉴런(Hidden Unit)입니다.\n",
    "\n",
    "**$O1, O2$** : 각각의 출력 뉴런을 의미합니다\n",
    "\n",
    "*활성화 함수(Activation function)* : 해당 그림에는 표기되어있지 않으나, 활성화 함수는 `Sigmoid`입니다.<br/>\n",
    "`Sigmoid`함수는 추후에 설명하겠으나, 흐름을 따라가기 위해서 기본적인 수식이 이렇다는 것만 알아두고 넘어가겠습니다.<br/>\n",
    "\n",
    "$$Sigmoid = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "이제부터 전파 알고리즘이 어떻게 진행되는지, 차례차레 풀어보도록 하겠습니다.\n",
    "\n",
    "기본적으로 인공 신경망의 연산은 다음과 같은 수식을 띕니다.\n",
    "\n",
    "<br/>\n",
    "$$Y = \\sigma(WX+b)$$\n",
    "\n",
    "<br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "\n",
    "수식이 어렵게 느껴지실 수도 있지만, 정말 간단합니다.\n",
    "\n",
    "$W$ : 가중치를 의미합니다.\n",
    "\n",
    "$X$ : 입력값을 의미합니다.\n",
    "\n",
    "$b$ : 편향(bias)라고 불리는 숫자(상수)입니다.\n",
    "\n",
    "$\\sigma$ : 활성화 함수(Acitvation function)을 의미합니다\n",
    "\n",
    "$Y$ : 출력값입니다.\n",
    "\n",
    "이를 가지고 말로 풀어서 써보자면, \n",
    "- 입력과 가중치를 곱합니다.\n",
    "- 그 결과에 어떠한 숫자(편향)를 더합니다.\n",
    "- 그 결과를 활성화 함수에 넣어줍니다.\n",
    "- 활성화 함수의 결과가 출력값이 됩니다.\n",
    "\n",
    "이 수식은 결국 맨 처음 이야기한 인공 신경이 하는 역활(다른 신경으로부터 신호를 받아서 종합하는 것)과 같다는 것을 알 수 있습니다.\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "**<font size=\"4\">1. $H1, H2, H3$ 뉴런 Input값과 Output값 유도**\n",
    "<br/><br/>\n",
    "그럼 이제 $H1$의 결과가 어떻게 나오게 됬는지 $Y = \\sigma(WX+b)$을 통해서 확인해보겠습니다.\n",
    "    \n",
    "\n",
    "- $Z = (l1 \\times 0.3) + (l2 \\times 0.2) + 0 = (10\\times0.3) + (20\\times0.2) + 0 = 7$ :: 여기에서 bias는 없으므로 0으로 대체합니다.\n",
    "\n",
    "\n",
    "- $Sigmoid(Z) = \\frac{1}{1+e^{-7}} = 0.999$\n",
    "<br/><br/>\n",
    "\n",
    "이런 형태로 $H2, H3$ 그리고 더 나아가서 $O1, O2$값도 똑같은 방법으로 구할 수 있습니다.\n",
    "\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Forward Operation :: matrix concept\n",
    "<br/>\n",
    "지금까지 기본적인 $Y = \\sigma(WX+b)$를 이용해서, 어떻게 MLP의 전파 알고리즘이 진행되는지 확인하였습니다\n",
    "\n",
    "이번에는 이를 모두 하나의 행렬로 묶어서 표현하고 연산하는 방법에 대해서 소개하겠습니다.\n",
    "\n",
    "Figure 6의 값들은 모두 행렬로 표현할 수 있고, 행렬 연산으로 수행할 수 있습니다.\n",
    "\n",
    "**행렬 연산으로 푸는 이유는 프로그래밍으로 행렬연산은 병렬처리가 가능하며, 수학적으로도 수식 전체가 간소화되는 이점이 있기 때문입니다.**\n",
    "\n",
    "<br/><br/>\n",
    "먼저 입력에 대해서 먼저 보면 $l1 = 10,  l2 = 20$ 입니다. \n",
    "\n",
    "<br/>\n",
    "이를 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$Input = \\begin{bmatrix}\n",
    "    10 & 20\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "Input layer와 Hidden Layer 사이의 가중치들을 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$W_{input}^{hidden} = \\begin{bmatrix}\n",
    "    0.3  & -0.1  &  1.1 \\\\\n",
    "    0.2  & -0.2  &  -0.5\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "Hidden Layer와 Output layer 사이의 가중치들을 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$W_{hidden}^{out} = \\begin{bmatrix}\n",
    "    1.1  &  0.4 \\\\\n",
    "    0.5  &  0.3 \\\\\n",
    "    0.7  &  0.2\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "<br/><br/>\n",
    "이렇게 데이터들을 행렬로 표현하고, 위와 똑같이 $Y = \\sigma(WX+b)$를 적용하면 더 간편하게 표현할 수 있습니다.\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "    \n",
    "여기서의 곱셈 연산은 행렬곱셈 연산을 의미합니다.<br/>\n",
    "행렬 곱셈 연산은 다음 Figure 8와 같은 방식으로 이루어집니다.<br/>\n",
    "*<font color=\"red\">행렬안의 내용이 어떠한 내용인지는 모르셔도 됩니다. 단지 행렬곱이 어떠한 순서로 이루어지는지만 보시면 됩니다.*<br/><br/>\n",
    "<img src=\"images/animation03.gif\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 8</u>: Matrix Multiply</center></caption>\n",
    "\n",
    "\n",
    "    \n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\">1. $H1, H2, H3$ 뉴런값들을 행렬 연산으로 유도** <br/>\n",
    "    \n",
    "계속 이야기했듯이 행렬표현으로 변경해도 전파 알고리즘 수식인 $Y = \\sigma(WX+b)$가 똑같이 적용된다고 말씀드렸었습니다.\n",
    "\n",
    "그럼 지금부터 행렬 연산을 기반으로 $H1, H2, H3$ 뉴런값을 한번에 구해보도록 하겠습니다.\n",
    "\n",
    "\n",
    "- $Z=WX+b$<br/><br/>\n",
    "$Z_{hidden} = \\begin{bmatrix}H1&H2&H3\\end{bmatrix} = \\begin{bmatrix}10&20\\end{bmatrix}\\times\\begin{bmatrix}0.3&-0.1&1.1\\\\0.2&-0.2&-0.5\\end{bmatrix} + \\begin{bmatrix}0&0&0\\end{bmatrix} = \\begin{bmatrix}7&-5&1\\end{bmatrix}$<br/><br/><br/>\n",
    "\n",
    "- $Y_{hidden} = Sigmoid(Z_{hidden})$<br/><br/>\n",
    "$ Y= \\begin{bmatrix}Sigmoid(H1)&Sigmoid(H2)&Sigmoid(H3)\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{1+e^{-7}}&\\frac{1}{1+e^{5}}&\\frac{1}{1+e^{-1}}\\end{bmatrix} = \\begin{bmatrix}0.999&0.007&0.731\\end{bmatrix}$<br/>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이런 방식으로 똑같이 $O1, O2$도 구할 수 있습니다.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Activation Function\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이전에 인공신경망을 설명드리면서 뉴런이 외부 뉴런으로 받은 신호들을 종합해서 다음 신경에게 신호를 보낼지 말지를 결정한다고 이야기했습니다.\n",
    "\n",
    "이를 인공신경망에서 구현하기 위해서 Activation Function을 사용하게됩니다.\n",
    "\n",
    "초기의 Step Function이라는 불연속적인 신호를 통해서 어떠한 임계값을 넘었을 때, 출력값을 보내고 임계점을 넘지 않았을 때는 \n",
    "\n",
    "출력값을 내보내지 않게하는 함수를 사용했습니다.\n",
    "\n",
    "Step Function은 Figure 9에 나타나있고, 수식은 다음과 같습니다.\n",
    "\n",
    "$$Step = \\begin{cases}\n",
    "    1 & \\mbox{if}& x \\in A, \\\\\n",
    "    0 & \\mbox {if}& x \\notin A. \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "<img src=\"images/step_fucntion.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center> <u>Figure 9</u>: Step Function</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "하지만 이런 `Step Function`은 나중에 인공 신경망을 학습 시킬 때, 사용할 수 없는 단점이 있습니다.\n",
    "\n",
    "나중에 더 자세하게 설명하겠지만, 이유를 간단히 말씀드리자면, 인공신경망을 학습할 때, 경사하강법을 통해서 학습하게 되는데,\n",
    "\n",
    "경사하강법은 미분을 사용합니다. 하지만 `Step Function`은 미분이 불가능한 함수이기 때문에, `Step Function`을 사용하게되면,\n",
    "\n",
    "경사하강법을 사용할 수가 없습니다.\n",
    "\n",
    "따라서 사람들은 경사하강법을 적용하기 위해서 `Activation Function`을 `Sigmoid Function`로 사용하게됩니다.\n",
    "\n",
    "`Step Function` 대신에 `Sigmoid Function`을 사용하는 이유는 `Sigmoid Function`이 `Step Function`과 비슷한 미분이 가능한 함수이기 때문입니다.\n",
    "\n",
    "`Sigmoid Function`의 그림은 Figure 10에 잘 나타나있으며, 수식은 다음과 같습니다.\n",
    "\n",
    "$$Sigmoid = $$\n",
    "\n",
    "<img src=\"images/sigmoid_1.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center> <u>Figure 10</u>: Sigmoid Function</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 Cost Function\n",
    "<br/><br/>\n",
    "지금까지 MLP의 전파 알고리즘(Forward Operation)의 개념을 확인하였습니다. \n",
    "\n",
    "이렇게 MLP의 전파 알고리즘을 타고 최종적으로 출력된 값은 중요한 의미를 갖습니다.\n",
    "\n",
    "예를 들어 어떠한 이미지를 입력데이터로 인공신경망에게 넣었을 때, 출력값은 이게 **개**인지 **고양이**인지에 대한 출력으로 볼 수 있습니다.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "만약에 우리가 가중치값들을 임의로 준 상태에서 입력에 고양이 이미지를 넣는다면 \n",
    "\n",
    "출력값은 분명 정확히 이게 개인지 고양인지 출력값을 낼 수 없을 것입니다.\n",
    "<br/><br/>\n",
    "\n",
    "그렇기 때문에 우리는 인공 신경망에게 입력 데이터를 주면서 이 데이터는 **고양이**야 라고 알려주어야 하며, 인공 신경망은 이를 통해서\n",
    "\n",
    "내가 낸 출력값과 정답이 얼만큼 차이가 나는지를 계산해서 스스로 가중치의 값을 변화하는 과정을 거치게 됩니다.\n",
    "\n",
    "이러한 학습 방법을 딥러닝에서는 지도 학습(Supervised Learning)이라고 부릅니다.\n",
    "\n",
    "\n",
    "<img src=\"images/cat-dog-flow-horizontal.gif\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 11</u>: Cat and Dog Classifier with Neural Network</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "그러면 임의로 가중치의 값이 주어진 인공신경망이 입력데이터를 받아서 출력값을 냈을 때, 정답과 출력값이 다른지는 어떻게 알며,\n",
    "\n",
    "얼마나 틀렸는지에 대해서는 어떻게 알까요?\n",
    "\n",
    "이러한 부분에 대해서 중요한 역활을 하는 것이 비용 함수(Cost Function)이 됩니다.\n",
    "\n",
    "그럼 이번에는 인공 신경망에서 쓰이는 비용 함수에 대해서 본격적으로 이야기 해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\">Cross-Entropy Error** <br/><br/>\n",
    "\n",
    "먼저 해당 스크립트에서는 비용 함수(Cost Function)를 Cross-Entropy Error를 사용합니다.\n",
    "\n",
    "Cross-Entory Error에 대한 수식은 다음과 같습니다.\n",
    "\n",
    "이해가 어려우시다면 뒤에 설명이 나오니 수식이 이렇게 생겼다는 것만 확인하시고, 넘어가시면 됩니다.\n",
    "\n",
    "$$ J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
    "\n",
    "**<font size=\"3\">오차(Error)** <br/><br/>\n",
    "    \n",
    "만약에 인공 신경망이 다음과 같은 2가지 결과를 주었다고 가정해 보겠습니다.\n",
    "\n",
    "우리는 여기서 인공 신경망의 정확도가 몇이고, 얼마나 틀렸는지를 확인해야한다고 합시다\n",
    "<br/><br/><br/>\n",
    "\n",
    "**1-th network**\n",
    "<table style=\"width:50%\", title=\"1-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.3  0.4 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "**2-th network**\n",
    "<table style=\"width:50%\", title=\"2-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.7  0.2 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br/>\n",
    "위 결과에 대해서 3가지 방법(**분류 오차/MSE(Mean Squared Error)/CEE(Cross-Entroy Error)** 오차를 계산해보겠습니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "**1. 분류 오차**\n",
    "\n",
    "분류 오차는 인공 신경망의 출력값들이 정답과 비교했을 때, 몇개를 맞췄고 몇개를 못맞췄는지에 대해서 비교해서\n",
    "\n",
    "인공신경망의 정확도를 측정하는데 쓰입니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "- *1-th network*:\n",
    " - 분류오차 -> *(3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)*$$\\frac{1}{3} = 0.33$$ <br/><br/><br/> \n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:\n",
    " - 분류오차 -> *(3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)* $$\\frac{1}{3} = 0.33$$ <br/><br/><br/>\n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "\n",
    "두 오차에 대해서 비교해보면, **결과는 같지만**, 2-th 네트워크의 첫 두 샘플은 1-th 네트워크보다 조금 더 확실히 맞추었고, 세번째 샘플은 아깝게 틀렸다는 것을 확인할 수 있습니다.\n",
    "\n",
    "이러한 결과를 보았을 때, **단순 분류 오차**의 계산은 틀린 개수에 대한 결과만 줄 뿐 **정답과 비교해서 얼마나 많이 틀렸는지 얼마나 정확하게 맞았는지** 그 정도에 대한 값을 제공하지 않습니다\n",
    "<br/><br/><br/><br/>\n",
    "**2. MSE(Mean Squared Error)**\n",
    "<br/><br/><br/>\n",
    "MSE(Mean Squared Error)는 정답과 내가 얼마나 멀리 떨어져있는지에 대해서 정의하는 함수입니다.\n",
    "\n",
    "쉽게 이야기하기 위해서 내가 집에 가고 싶은데, 지금 위치에서 집까지의 거리를 확인해서 \n",
    "\n",
    "내가 집까지 가기 위한 거리값을 오차값으로 잡는 것을 MSE로 이해하면됩니다.\n",
    "\n",
    "MSE의 수식은 다음과 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "$$J = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{(\\hat{Y_{i}} - Y_{i})^{2}}$$\n",
    "\n",
    "<img src=\"images/scatter_plot.gif\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u>Figure 12</u>: Mean Squared Error</center></caption>\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:<br/><br/>\n",
    " - 분류오차 -> $(0.3-0)^{2} + (0.3-0)^{2} + (0.4-1)^{2} = 0.54$ (나머지 2개의 샘플에 대해서는 생략..)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{(0.54 + 0.54 + 1.34)}{3} = 0.81$<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:<br/><br/>\n",
    " - 분류오차 -> (생략...)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{(0.14 + 0.14 + 0.74)}{3} = 0.34$<br/><br/>\n",
    "\n",
    "<br/><br/>\n",
    "**3. Cross-entropy Error**\n",
    "\n",
    "\n",
    "CEE(Cross-Entropy Error)는 내 출력값과 정답값이 얼마나 틀렸는지도 확인하고, 정답이 아닌 부분에 대해서도 이걸 정답으로\n",
    "\n",
    "출력했다면, 이를 같이 반영하는 방법입니다.\n",
    "\n",
    "<br/><br/>\n",
    "수식은 다음과 같습니다. \n",
    "\n",
    "<br/>\n",
    "$$J = -\\frac{1}{m}\\sum\\limits_{i = 1}^{m}(\\hat{Y_{i}}\\log\\left(Y_{i}\\right) + (1-\\hat{Y_{i}})\\log\\left(1- Y_{i}\\right))$$\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:<br/><br/>\n",
    "\n",
    " - 분류오차 -> (나머지 2개의 샘플에 대해서는 생략..)<br/><br/>$ -((0\\times\\log\\left(0.3\\right) + (1-0)\\times\\log\\left(1-0.3\\right)) + 0\\times\\log\\left(0.3\\right) + (1-0)\\times\\log\\left(1-0.3\\right) + 1\\times\\log\\left(0.4\\right) + (1-1)\\times\\log\\left(1-0.4\\right))$<br/><br/>\n",
    " = $(-0.356)+(-0.356)+(-0.916) = -1.628$<br/><br/>\n",
    " \n",
    " - 분류 정확도 -> <br/><br/>\n",
    " $\\frac{-(-2.472 + -3.723 + -1.628)}{3} = 2.607$<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:<br/><br/>\n",
    " - 분류오차 -> (생략...)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{-(-0.684 + -0.684 + -2.066)}{3} = 1.144$<br/><br/>\n",
    " \n",
    " \n",
    "<br/><br/>\n",
    "결과에서 볼 수 있듯이 MSE는 틀린 샘플에 더 집중하는 특성을 갖습니다.\n",
    "\n",
    "맞은 것과, 틀린 것에 똑같이 집중해야 하는데, 그렇지 않아 비용 함수로 정의할 때, 부적절하다고 할 수 있습니다.\n",
    "\n",
    "따라서, 해당 스크립트의 비용 함수는 Cross-entory Error를 사용합니다.\n",
    "\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 경사하강법 (Gradient Descent Algorithm)\n",
    "<br/><br/>\n",
    "\n",
    "이번에는 인공 신경망에서 학습할 때, 쓰이는 경사하강법에 대해서 알아보겠습니다.\n",
    "\n",
    "경사하강법은 특정 함수에서 제일 작은 값을 찾아가는 알고리즘입니다.\n",
    "\n",
    "아까 위의 설명에서 비용함수에 대해서 이야기했었습니다.\n",
    "\n",
    "해당 비용함수의 수식은 에러에 대한 수식인데, 이러한 비용함수는 하나의 오차에 대한 3차원 표면으로 나타날 수 있습니다.\n",
    "\n",
    "이러한 3차원 표면을 에러 표면이라고 하며, 이러한 에러 표면은 Figure 13에 잘 나타나있습니다.\n",
    "\n",
    "경사하강법은 이러한 3차원 표면에서 제일 낮은 값. 즉, Figure 13에서 `Global Minima`라고 적힌\n",
    "\n",
    "제일 낮은곳으로 내려가게끔 만들어주는 알고리즘입니다.\n",
    "\n",
    "<img src=\"images/error_surface.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 13</u>: Error Surface </center></caption>\n",
    "\n",
    "\n",
    "경사하강법은 이러한 에러 표면에서 에러값이 제일 작은 곳을 찾아가는 알고리즘입니다. \n",
    "\n",
    "경사하강법의 직관적인 그림은 Figure 14에 잘 나타나있습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"images/sgd.gif\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 14</u>: Gradient Descent Algorithm </center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "경사하강법의 수식은 다음과 같습니다.\n",
    "\n",
    "**<font size=\"4\">$$x_{i+1} = x_{i} - \\gamma_{i}\\nabla{f(x_{i})}$$<br/>**\n",
    "    \n",
    "\n",
    "해당 수학 기호의 의미는 다음과 같습니다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "    \n",
    "$x_{i+1}$ : 다음 $x$의 값\n",
    "\n",
    "$x_{i}$ : 현재 $x$의 값\n",
    "\n",
    "$\\gamma_{i}$ : 학습율(learning rate) -> 학습율은 사람이 값을 임의로 설정해줘야하는 값입니다.\n",
    "\n",
    "\n",
    "$\\nabla{f(x_{i})}$ : $f(x_{i})$에서의 미분값 (현재 $x_{i}$의 함수값의 미분값) \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "그럼, 이제부터 `MSE` 비용 함수에 경사하강법 알고리즘을 적용해서 최소점을 찾아가는 방법을\n",
    "\n",
    "수식으로 풀어가면서 이야기하도록 하겠습니다.\n",
    "\n",
    "비용함수 챕터에서 `MSE` 수식을 언급했었습니다.\n",
    "\n",
    "**<font size=\"3\">$$J = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{(\\hat{Y_{i}} - Y_{i})^{2}}$$**\n",
    "\n",
    "여기서 $\\sum$와, $\\frac{1}{n}$를 빼고 수식 $(\\hat{Y_{i}} - Y_{i})^{2}$만 잘 보면, 결국에 정답 $\\hat{Y_{i}}$은 정해져있고, \n",
    "\n",
    "네트워크의 출력값 $Y_{i}^{2}$을 정답에 맞추면 비용함수가 값이 `0`이 된다는 것을 알 수 있습니다.\n",
    "\n",
    "위의 `MSE`함수를 우리 모두가 아는 일반적인 함수로 바꿔보겠습니다.\n",
    "\n",
    "$y = (x-c)^{2}$\n",
    "\n",
    "위의 함수 어디서 많이 본 거 같지 않나요? 바로 2차 함수에 대한 식입니다.<br/><br/>\n",
    "\n",
    "<img src=\"images/capture2-1.gif\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 15</u>: quadratic function </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "우리는 이러한 2차 함수 형태를 지니는 임의의 `MSE`함수를 정의하고, 경사하강법을 적용해서 `MSE`함수의 값을 0으로 수렴시켜보도록 하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이제 임의의 `MSE`함수를 아래와 같이 정의하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$y = x^{2}$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이러한 이차함수 `MSE`의 미분값은 아래와 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$y^{'} = 2x$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "이제 경사하강법을 시작하기 위해서 랜덤한 x값을 주고, 여기에서부터 시작해서 경사하강법을 진행해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이번 예제에서 경사하강법의 `학습률(learning rate)`은 `0.1`을 갖는다고 생각하고 진행하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "랜덤한 값 **$x$** 를 `3`으로 줬다고 가정합시다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "그럼 $y$값은 다음과 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$y=3^{2} = 9$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "그럼 이제 여기에서 경사하강법을 적용해봅시다.\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "경사하강법의 수식은 위에서 이야기했듯이 아래와 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\">$$x_{i+1} = x_{i} - \\gamma_{i}\\nabla{f(x_{i})}$$**\n",
    "<br/><br/>\n",
    "위의 수식을 그대로 따라서 적용해보면 다음과 같습니다.\n",
    "<br/><br/>\n",
    "\n",
    "- 1번째 스텝<br/><br/>\n",
    "$x_{1} = 3 - 0.1 \\times y^{'} = 3 - 0.1 \\times 2x = 3 - 0.1 \\times 6 = 3-0.6 = 2.4$<br/><br/>\n",
    "\n",
    "- 2번째 스텝<br/><br/>\n",
    "$x_{2} = 2.4 - 0.1 \\times y^{'} = 2.4 - 0.1 \\times 2x = 2.4 - 0.1 \\times 4.8 = 2.4-0.48 = 1.92$<br/><br/>\n",
    "\n",
    "- 3번째 스텝<br/><br/>\n",
    "$x_{3} = 1.92 - 0.1 \\times y^{'} = 1.92 - 0.1 \\times 2x = 1.92 - 0.1 \\times 3.84 = 1.92-0.384 = 1.536$<br/><br/>\n",
    "\n",
    "\n",
    "확인해보시면 결과가 계속해서 줄어는 것을 확인할 수 있습니다.\n",
    "\n",
    "이렇게 계속 스텝을 진행하다보면 함수의 값이 점점 `0`에 가까워지는 것을 확인하실 수 있습니다.<br/>\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/><br/><br/><br/>\n",
    "    \n",
    "- 학습할 때, `학습률(learing rate)`의 값은 항상 신경써서 맞춰줘야합니다. `학습률`을 너무 낮게 주면, 학습하는데 큰 시간이 걸리고, `학습률`을 너무 크게주면, 학습이 안될 수 있습니다. 아래 Figure 16은 `학습률(learning rate)`을 잘못 주었을 때, 어떤 현상이 나타나는지 보여주는 그림입니다.\n",
    "<br/><br/>\n",
    "<img src=\"images/sgd_bad.gif\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 14</u>: Bad Learning rate </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5 역 전파 알고리즘(Backpropagation)\n",
    "\n",
    "<br/>\n",
    "\n",
    "그럼 위에 이야기한 경사하강법을 어떻게 인공 신경망에 적용하는지에 대해서 알아보겠습니다.\n",
    "\n",
    "다시 한번 간단한 인공신경망의 구조를 확인하고 넘어가도록 하겠습니다.\n",
    "\n",
    "<img src=\"images/neural_network-7.png\" style=\"width:500px;height:400px;\">\n",
    "<caption><center> <u>Figure 15</u>: Artificial Neural Network </center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "<br/>\n",
    "역 전파 알고리즘(Backpropagation)은 `체인 룰(Chain rule)` 이라는 것에 기반을 두고 있습니다.\n",
    "\n",
    "`체인 룰(Chain rule)`하면 뭔가 딱 떠오르는게 없나요? 우리 많이 하는 게임 보면 `체인 라이트닝`이라는 스킬을 많이 볼 수 있습니다.\n",
    "\n",
    "`체인 룰(Chain rule)`은 바로 이 `체인 라이트닝`과 같은 역활을 합니다.\n",
    "\n",
    "첫 한방만 맞으면 연쇄반응을 일으키면서 뒤로 전파되는 것이죠.\n",
    "<br/><br/><br/>\n",
    "<img src=\"images/chain_lightning.jpg\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u>Figure 16</u>: Chain Lightning </center></caption>\n",
    "<br/><br/>\n",
    "딥러닝에서는 학습하는 역 전파 알고리즘을 이러한 `체인 룰(Chain rule)` 통해서 학습하게 됩니다.\n",
    "\n",
    "무수히 연결된 신경들에게 `데이터가 입력된 방향`으로 신경 노드들 개별의 `미분값`을 가지고있으면 \n",
    "\n",
    "이들을 `인공 신경망의 출력`에서 `데이터가 입력된 방향`으로 개별의 신경 노드들의 `미분값`들을 곱해가면서\n",
    "\n",
    "위에서 설명한 경사하강법(Gradient Descent)를 적용해서 `파라미터`들을 업데이트해주는 것을 **`체인 룰(Chain rule)`** 이라고 합니다.\n",
    "\n",
    "결국에 Figure 16과 같이 `소서리스`의 `체인 라이트닝`과 같은 스킬을 딥러닝이 쓰게 되는 것입니다.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "<img src=\"images/output_1_backprop-4.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> <u>Figure 17</u>: Back Propagation </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "<img src=\"images/neural_network-7.png\" style=\"width:700px;height:600px;\">\n",
    "<caption><center> <u>Figure 18</u>: Artificial Neural Network </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "결국 여기서 우리가 경사하강법 알고리즘을 통해서 업데이트하고싶은 값은 가중치(Weights)값입니다.\n",
    "\n",
    "Figure 18에서 가중치 값은 다음과 같은 행렬값입니다.\n",
    "\n",
    "<br/>\n",
    "$W^{1} = \\begin{bmatrix}\n",
    "    W^{1}_{11}  & W^{1}_{21}\\\\\n",
    "    W^{1}_{12}  & W^{1}_{22}\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "$W^{2} = \\begin{bmatrix}\n",
    "    W^{2}_{11}  & W^{2}_{21}\\\\\n",
    "    W^{2}_{12}  & W^{2}_{22}\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/><br/>\n",
    "  \n",
    "<font size=\"3\">$W^{a}_{bc}$<br/>\n",
    "\n",
    "- $a$ : 인공 신경망에서 레이어 순서를 의미\n",
    "- $bc$ : 인공 신경망에서 b번째 노드에서 c번째 노드로 향함을 의미\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**ex>** <br/>\n",
    "\n",
    "- $W^{1}_{11}$ : 첫번째 레이어의 신경망에서 1번째(상단)노드에서 다음 레이어의 1번째 노드(상단)로 향하는 가중치를 의미합니다\n",
    "    \n",
    "\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/backpropagation.png\" style=\"width:700;height:600px;\">\n",
    "<caption><center> <u>Figure 19</u>: Back Propagation </center></caption>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "이제 그럼 이를 직접 풀어보도록 하겠습니다. 여기서는 문제를 조금 쉽게 풀기위해서 `비용 함수(Cost Function)`은 `MSE`로 하고,\n",
    "`learning rate`는 `0.5`로 하겠습니다. $O_{1}$의 정답은 `0.01`이고, $O_{2}$의 정답은 `0.99`로 가정합니다.\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/neural_network-9.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> <u>Figure 20</u>: Artificial Neural Network </center></caption>\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "$I = \\begin{bmatrix} 0.05 & 0.1 \\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$W^{1} = \\begin{bmatrix} 0.15 & 0.25\\\\0.20 & 0.30 \\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$b^{1} = \\begin{bmatrix} 0.35\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$W^{2} = \\begin{bmatrix} 0.40 & 0.50\\\\0.45 & 0.55 \\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "$b^{2} = \\begin{bmatrix} 0.60\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "**<font size=\"4\"> 1. 전파 알고리즘** \n",
    "    \n",
    "이제 가중치들을 이용해서 전파 알고리즘을 수행합니다.\n",
    "\n",
    "전파 알고리즘을 수행해서 우리는 $O_{1}$과 $O_{2}$의 값을 구할 수 있습니다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**1-1. $Z^{1}$** = $IW^{1} + b^{1} = \\begin{bmatrix} 0.05 & 0.1 \\end{bmatrix} \\times \\begin{bmatrix} 0.15 & 0.25\\\\0.20 & 0.30 \\end{bmatrix} + \\begin{bmatrix} 0.35\\end{bmatrix} = \\begin{bmatrix} 0.0275 & 0.0425 \\end{bmatrix} + \\begin{bmatrix} 0.35\\end{bmatrix} = \\begin{bmatrix} 0.3755 & 0.3925 \\end{bmatrix} $\n",
    "\n",
    "**1-2. $A^{1}$** = $Sigmoid(Z^{1}) = \\begin{bmatrix} Sigmoid(0.3755) & Sigmoid(0.3925) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{1+e^{-0.3755}} & \\frac{1}{1+e^{-0.3925}}\\end{bmatrix} = \\begin{bmatrix} 0.593 & 0.597 \\end{bmatrix}$\n",
    "\n",
    "**1-3. $Z^{2}$** = $A^{1}W^{2} + b^{2} = \\begin{bmatrix} 0.593 & 0.597 \\end{bmatrix} \\times \\begin{bmatrix} 0.40 & 0.50\\\\0.45 & 0.55 \\end{bmatrix} + \\begin{bmatrix} 0.60\\end{bmatrix}= \\begin{bmatrix} 1.106 & 1.224 \\end{bmatrix}$\n",
    "\n",
    "**1-4. $A^{2}$** = $Sigmoid(Z^{2}) = \\begin{bmatrix} Sigmoid(1.106) & Sigmoid(1.224) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{1+e^{-1.106}} & \\frac{1}{1+e^{-1.224}}\\end{bmatrix} = \\begin{bmatrix} 0.751 & 0.772 \\end{bmatrix}$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\"> 2. 비용 측정** \n",
    "    \n",
    "전파 알고리즘을 통해 나온 결과값과 실제 정답셋과의 차이(비용)을 `MSE`함수를 통해서 측정합니다.\n",
    "\n",
    "$$J = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{(\\hat{Y_{i}} - Y_{i})^{2}}$$\n",
    "<br/><br/>\n",
    "$J_{1} = (\\hat{Y_{i}} - Y_{i})^{2} = (0.01 - 0.751)^{2} = 0.549081$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$J_{2} = (\\hat{Y_{i}} - Y_{i})^{2} = (0.99 - 0.772)^{2} = 0.047524$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$J_{total} =  \\frac{1}{2}(J_{1} + J_{2})= \\frac{1}{2}(0.549081 + 0.047524) = 0.596605$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\"> 3. 역 전파 알고리즘** \n",
    "    \n",
    "<br/>\n",
    "\n",
    "**<font size=\"3\"> 3-1. 체인 룰을 적용하기 위한 미분값 구하기** \n",
    "\n",
    "<br/>\n",
    "\n",
    "이제 Figure 21과 같이 각 노드들의 미분값들을 모두 구해보도록 하겠습니다.\n",
    "\n",
    "시작하기전에 아래와 같은 수식이 나올 것 입니다.\n",
    "\n",
    "$$\\frac{\\partial A}{\\partial B}$$\n",
    "\n",
    "해당 수식은 `편미분`이라고 불리는 것입니다.\n",
    "\n",
    "위의 `편미분`식이 의미하는 것은 `A`에 `B`가 기여한 정도를 의미합니다.\n",
    "\n",
    "앞으로 나올 미분하는 방법에 대해서는 미분을 잘 모르신다면, 어떻게 미분이 되는지 자세히 모르셔도 됩니다.\n",
    "그냥 이 식을 미분하면 이렇게 미분이 된다는 것만 아시면 됩니다.\n",
    "\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/backpropagation.png\" style=\"width:700;height:600px;\">\n",
    "<caption><center> <u>Figure 21</u>: Back Propagation </center></caption>\n",
    "<br/>\n",
    "\n",
    "$J = \\frac{1}{2}\\sum\\limits_{i=1}^{2}{(\\hat{Y_{i}} - Y_{i})^{2}}$<br/>\n",
    "$J = \\frac{1}{2}((\\hat{Y_{1}} - Y_{1})^{2}) + (\\hat{Y_{2}} - Y_{2})^{2})$\n",
    "<br/>\n",
    "- $\\frac{\\partial J}{\\partial O_{1}} = 2 * \\frac{1}{2}(\\hat{Y_{1}} - O_{1})^{2-1} \\times -1 + 0 = -(\\hat{Y_{1}} - O_{1})$\n",
    "<br/>\n",
    "- $\\frac{\\partial J}{\\partial O_{2}} = 2 * \\frac{1}{2}(\\hat{Y_{2}} - O_{2})^{2-1} \\times -1 + 0 = -(\\hat{Y_{2}} - O_{2})$\n",
    "\n",
    "<br/><br/>\n",
    "$Sigmoid = \\frac{1}{1+e^{-x}}$<br/>\n",
    "$Sigmoid^{'} = Sigmoid(1-Sigmoid) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}})$\n",
    "<br/>\n",
    "- $\\frac{\\partial O_{1}}{\\partial Z^{2}_{1}} =  O_{1}(1-O_{1})$\n",
    "<br/>\n",
    "- $\\frac{\\partial O_{2}}{\\partial Z^{2}_{2}} =  O_{2}(1-O_{2})$\n",
    "\n",
    "<br/><br/>\n",
    "$Z = WX + b$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{2}}{\\partial W_{11}^{2}} = \\frac{\\partial(W_{11}^{2}A_{1}^{1} + W_{21}^{2}A_{1}^{2} + b^{2})}{\\partial W_{11}^{2}} = A_{1}^{1}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{2}}{\\partial W_{21}^{2}} = \\frac{\\partial(W_{11}^{2}A_{1}^{1} + W_{21}^{2}A_{1}^{2} + b^{2})}{\\partial W_{21}^{2}} = A_{1}^{2}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{2}^{2}}{\\partial W_{12}^{2}} = \\frac{\\partial(W_{12}^{2}A_{1}^{1} + W_{22}^{2}A_{1}^{2} + b^{2})}{\\partial W_{12}^{2}} = A_{1}^{1}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{2}^{2}}{\\partial W_{22}^{2}} = \\frac{\\partial(W_{12}^{2}A_{1}^{1} + W_{22}^{2}A_{1}^{2} + b^{2})}{\\partial W_{22}^{2}} = A_{1}^{2}$\n",
    "\n",
    "<br/><br/>\n",
    "$Sigmoid = \\frac{1}{1+e^{-x}}$<br/>\n",
    "$Sigmoid^{'} = Sigmoid(1-Sigmoid) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}})$\n",
    "<br/>\n",
    "- $\\frac{\\partial A_{1}^{1}}{\\partial Z^{1}_{1}} =  A_{1}(1-A_{1})$\n",
    "<br/>\n",
    "- $\\frac{\\partial A_{2}^{1}}{\\partial Z^{1}_{2}} =  A_{2}(1-A_{2})$\n",
    "\n",
    "<br/><br/>\n",
    "$Z = WX + b$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}} = \\frac{\\partial(W_{11}^{1}I_{1} + W_{21}^{1}I_{2} + b^{1})}{\\partial W_{11}^{1}} = I_{1}$\n",
    "<br/>\n",
    "- $\\frac{\\partial Z_{1}^{1}}{\\partial W_{21}^{1}} = \\frac{\\partial(W_{11}^{1}I_{1} + W_{21}^{1}I_{2} + b^{1})}{\\partial W_{21}^{1}} = I_{2}$\n",
    "<br/>\n",
    "-  $\\frac{\\partial Z_{2}^{1}}{\\partial W_{12}^{1}} = \\frac{\\partial(W_{12}^{1}I_{1} + W_{22}^{1}I_{2} + b^{1})}{\\partial W_{12}^{1}} = I_{1}$\n",
    "<br/>\n",
    "-  $\\frac{\\partial Z_{2}^{1}}{\\partial W_{22}^{1}} = \\frac{\\partial(W_{12}^{1}I_{1} + W_{22}^{1}I_{2} + b^{1})}{\\partial W_{22}^{1}} = I_{2}$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "이제 모든 미분값들을 구했으니, 이 미분값들을 이용해서 경사하강법 알고리즘을 통해서 가중치값을 업데이트하는 과정을 보겠습니다.\n",
    "\n",
    "예를 들어서 우리가 $W_{11}^{2}$의 가중치 값을 업데이트하고 싶다면, 인공신경망 출력부분에서 \n",
    "\n",
    "$W_{11}^{2}$까지 오는 `체인룰(Chain rule)`이라는 개념을 사용해서 모두 곱해주면 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial W_{11}^{2}} = \\frac{\\partial Z_{2}^{1}}{\\partial W_{11}^{2}} \\times \\frac{\\partial O_{1}}{\\partial Z_{2}^{1}} \\times \\frac{\\partial E}{\\partial O_{1}}$\n",
    "\n",
    "해당 수식을 아까 미분값을 구한 것으로 바꾸면 아래와 같이 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial W_{11}^{1}} = A^{1}_{1} \\times O_{1}(1-O_{1}) \\times (\\hat{Y_{1}} - O_{1})$\n",
    "\n",
    "여기서 $A^{1}_{1}$, $O_{1}$, $\\hat{Y_{1}}$등과 같은 숫자들은 이미 전파 알고리즘에서 우리가 다 구한 값입니다.\n",
    "\n",
    "따라서 그 결과들을 그래로 넣어서 계산을 하면 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial E}{\\partial W_{11}^{2}} = 0.593 \\times 0.187 \\times 0.741 = 0.082170231$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "자 이렇게 우리는 전체 비용(에러)에 대한 $W_{11}^{2}$의 기여도를 구했습니다.\n",
    "\n",
    "그럼 이제 경사하강 알고리즘을 통해서 $W_{11}^{2}$값을 업데이트 해보겠습니다.\n",
    "\n",
    "경사하강 알고리즘은 다음과 같은 수식을 따른다고 말했었습니다.\n",
    "\n",
    "<br/>\n",
    "**<font size=\"4\">$$x_{i+1} = x_{i} - \\gamma_{i}\\nabla{f(x_{i})}$$**\n",
    "<br/>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "그리고 우리는 학습률(learning rate)의 값 $\\gamma_{i}$을 `0.5`로 하기로 했었습니다.\n",
    "\n",
    "$\\nabla{f(x_{i})}$는 `미분값`이므로 전체 비용에 대한 $x$에 대한 기여도입니다.\n",
    "\n",
    "자 이제 우리가 여태까지 구했던 값들을 이용해서 새로운 $W_{11}^{1}$값을 구해봅시다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "$(W_{11}^{2})^{+} = (W_{11}^{2}) - 0.5 \\times \\frac{\\partial E}{\\partial W_{11}^{1}} = 0.40 - (0.5 \\times 0.082170231) = 0.358914885$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이렇게 우리는 2번째 레이어의 가중치값들을 다음과 같이 모두 업데이트할 수 있습니다.\n",
    "\n",
    "$(W_{21}^{2})^{+} = 0.408666186$\n",
    "\n",
    "\n",
    "$(W_{12}^{2})^{+} = 0.511301270$\n",
    "\n",
    "\n",
    "$(W_{22}^{2})^{+} = 0.561370121$\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "그럼 이제 1번째 레이어의 가중치들도 업데이트를 해보겠습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{11}^{1}} = \\frac{\\partial E}{\\partial A_{1}^{1}} \\times \\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} \\times \\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "여기서 위에서 구하지 않았던 새로운 수식 $\\frac{\\partial E}{\\partial A_{1}^{1}}$ 을 확인할 수 있습니다.\n",
    "\n",
    "해당 수식이 의미하는 바는 아래의 수식과 같습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial A_{1}^{1}} = \\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}} + \\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}}$$\n",
    "\n",
    "여기서 $\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}}$는 다음과 같습니다.\n",
    "아래의 숫자는 이미 위에서 모두 구해놨던 값을 참조하였습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}} = \\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}}\\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{2}} = 0.741 \\times 0.186 = 0.138$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "즉 신경노드 `h1의 출력값`이 전체 비용(에러)에 얼마만큼 기여했는지에 대한 내용입니다.\n",
    "\n",
    "우리가 2번째 레이어의 가중치를 업데이트할 때, 계산했었던 값들을 아래의 식에 대입하면 다음과 같습니다.\n",
    "\n",
    "$\\frac{\\partial E}{\\partial O_{1}}\\frac{\\partial O_{1}}{\\partial Z_{1}^{2}} = 0.741 \\times 0.186 = 0.138$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$\\frac{\\partial Z_{1}^{2}}{\\partial A_{1}^{1}}$은 $W_{12}^{2}$와 같습니다.\n",
    "\n",
    "$\\frac{\\partial Z_{1}^{2}}{\\partial A_{1}^{1}} = W_{12}^{2} = 0.40$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이를 조합하면 다음과 같습니다.\n",
    "\n",
    "$\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{2}} = \\frac{\\partial E_{O_{1}}}{\\partial Z_{1}^{2}}\\frac{\\partial Z_{1}^{2}}{\\partial A_{1}^{2}} = 0.138498562 * 0.40 = 0.055399425$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "위와 같이 $\\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}}$를 구하게되면, 아래와 같습니다.\n",
    "\n",
    "$$\\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}} = -0.019049119$$\n",
    "\n",
    "따라서 다음과 같은 결과를 얻을 수 있습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial A_{1}^{1}} =\\frac{\\partial E_{O_{1}}}{\\partial A_{1}^{1}} + \\frac{\\partial E_{O_{2}}}{\\partial A_{1}^{1}} = 0.055399425 + -0.019049119 = 0.036350306$$\n",
    "\n",
    "이제, 아래에서 $\\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}}$와 $\\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}}$를 구해보도록 하겠습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial A_{1}^{1}} \\times \\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} \\times \\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}}$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$\\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} = A_{1}^{1}(1-A_{1}^{1}) = 0.59326999(1-59326999) = 0.241300709$\n",
    "\n",
    "\n",
    "$\\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}} = I_{1} = 0.05$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "모두 종합해보면 아래와 같습니다.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_{11}^{1}} = \\frac{\\partial E}{\\partial A_{1}^{1}} \\times \\frac{\\partial A_{1}^{1}}{\\partial Z_{1}^{1}} \\times \\frac{\\partial Z_{1}^{1}}{\\partial W_{11}^{1}} = 0.036350306 \\times 0.241300709 \\times 0.05 = 0.000438568$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "마지막으로 경사하강법을 적용해 가중치값을 업데이트하면 다음과 같습니다.\n",
    "\n",
    "$(W_{11}^{1})^{+} = (W_{11}^{1}) - 0.5 \\times \\frac{\\partial E}{\\partial W_{11}^{1}} = 0.15 - (0.5 \\times 0.000438568) = 0.149780716$\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "이와 같은 방법으로 1번째 레이어의 가중치들을 업데이트하게되면, 다음과 같습니다.\n",
    "\n",
    "$(W_{12}^{1})^{+} = 0.24975114$\n",
    "\n",
    "$(W_{21}^{1})^{+} = 0.19956143$\n",
    "\n",
    "$(W_{22}^{1})^{+} = 0.29950229$\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "자! 이제 전반적인 MLP에 사용되는 알고리즘을 모두 살펴보았습니다.\n",
    "\n",
    "이제부터 Python의 numpy를 이용해서 위에서 설명한 알고리즘들을 모두 코드로 구현해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Forward Operation\n",
    "\n",
    "<br/>\n",
    "\n",
    "1. 먼저 입력 데이터의 차원에 맞게, 가중치 행렬을 생성하고 초기화합니다.\n",
    "2. 초기화된 Weights와 bias를 입력 데이터를 이용해서 $Y = WX + b$을 구합니다.\n",
    "3. ACTIVATION (Sigmoid, ReLU etc)를 이용해서 최종 출력을 얻습니다.\n",
    "\n",
    "\n",
    "<img src=\"images/perceptron.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 1</u>: Forward Operation. <br> The model can be summarized as: ***[INPUT -> LINEAR -> ACTIVATION]***</center></caption>\n",
    "\n",
    "<br/>\n",
    "위의 그림을 수식으로 일반화하면, 아래와 같습니다\n",
    "<br/><br/>\n",
    "\n",
    "입력데이터인 MNIST 데이터의 모양은 $(784, 128)$입니다. `784`는 총 픽셀의 개수이고, `128`은 한번에 학습시킬 이미지의 갯수가 `128`개라는 의미입니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "<br/>\n",
    "\n",
    "`Python`에서 `Numpy`를 사용해서 $W X + b$를 계산할 때, $b$는 자동으로 차원이 맞춰지는 broadcasting과정을 거칩니다.\n",
    "\n",
    "아래는 broadcasting이라는 과정을 보여줍니다 : \n",
    "\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "<br/><br/><br/>\n",
    "위와 같이 계산하면 $WX + b$ 아래와 같은 식이 됩니다 : \n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Initialization\n",
    "\n",
    "- 데이터 차원에 맞게 가중치 행렬을 생성하고 초기화합니다.\n",
    "- 일반적으로 weights는 난수로, bias는 0으로 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def init_params(input_dims, output_dims):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(output_dims, input_dims) * 0.1\n",
    "    b1 = np.zeros(shape=(output_dims, 1))\n",
    "    \n",
    "    assert(W1.shape == (output_dims, input_dims))\n",
    "    assert(b1.shape == (output_dims, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                 }\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.16243454 -0.06117564 -0.05281718 -0.10729686]]\n",
      "b1 = [[ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = init_params(2,2)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"].reshape(1,4)))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"].reshape(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.16243454 -0.06117564]\n",
    " [-0.05281718 -0.10729686]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Linear Forward Module\n",
    "\n",
    "Linear Forward Module은 아래와 같은 수식으로 일반화되어 계산됩니다.\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "여기서 $A^{[0]} = X$. <br/><br/>\n",
    "즉, $A^{[l-1]}$은 이전 레이어에서 온 출력값이고, $A^{[0]}$. 이전 레이어의 처음값은 입력 데이터라는 의미입니다.\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    print(\"{} x {}\".format(W.shape,A.shape))\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key of parameters : dict_keys(['b1', 'W1'])\n",
      "X's dimensions : (1, 784), W shape : (784, 10), b shape : (784, 1)\n",
      "Matrix Multiply : (1, 784) x (784, 10) = (1,10)\n",
      "Z Shape : (784, 10)\n",
      "\n",
      "\n",
      "Z = [-0.73182675 -0.83352089 -1.25611    -0.27407417 -1.98219577  0.05147004\n",
      "  1.08126451  0.73299607  0.60851156 -1.10198167]\n"
     ]
    }
   ],
   "source": [
    "# Input Data Reshape\n",
    "X = train_img[0].reshape(1,784)\n",
    "# Init Params\n",
    "parameters = init_params(10,784)\n",
    "A, W, b = X, parameters[\"W1\"], parameters[\"b1\"]\n",
    "\n",
    "print(\"key of parameters : {}\".format(parameters.keys()))\n",
    "print(\"X's dimensions : {}, W shape : {}, b shape : {}\".format(X.shape, W.shape, b.shape))\n",
    "print(\"Matrix Multiply : {} x {} = ({},{})\".format(A.shape, W.shape, A.shape[0], W.shape[1]))\n",
    "Z = np.dot(A, W) + b\n",
    "print(\"Z Shape : {}\".format(Z.shape), end=\"\\n\\n\\n\")\n",
    "#Z, linear_cache = linear_forward(X, W, b)\n",
    "\n",
    "print(\"Z = \" + str(Z[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td>[-0.73182675 -0.83352089 -1.25611    -0.27407417 -1.98219577  0.05147004\n",
    "  1.08126451  0.73299607  0.60851156 -1.10198167]</td> \n",
    "  </tr>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Linear-Activateion Forward\n",
    "\n",
    "이번 예제에서는 2가지 종류의 Activation function을 사용합니다.\n",
    "\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. <br/><br/> `sigmoid` 함수는 **2개**의 return 값을 갖습니다 : activation value \"`a`\", \"`Z`\"를 포함하고 있는 \"`cache`\" (Backpropagation때, 이를 사용합니다). <br/><br/>사용법은 다음과 같습니다: \n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "<img src=\"images/sigmoid.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 2</u>: Sigmoid.</center></caption>\n",
    "\n",
    "- **ReLU**: $A = ReLU(Z) = max(0, Z)$.<br/><br/> `relu` 함수는 **2개**의 return값을 갖습니다.: the activation value \"`A`\", \"`Z`\"를 포함하고 있는\"`cache`\" (Backpropagation때, 이를 사용합니다). <br/><br/>사용법은 다음과 같습니다:\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```\n",
    "<img src=\"images/relu.jpeg\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 3</u>: ReLU.</center></caption>\n",
    "\n",
    "- **Sigmoid의 미분**: <br/> $g'(Z) = \\frac{ d }{ dz }\\frac{ 1 }{ 1+e^{-z} } = \\frac{1}{(1+e^-z)^2}(e^{-z}) = \\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}}) = g(z)(1-g(z))$<br/><br/>\n",
    "\n",
    "- **ReLU의 미분**: <br/> $g'(Z) = \\max(0, Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(Z):\n",
    "    return (1/(1+np.exp(-Z)) * (1-1/(1+np.exp(-Z))))\n",
    "\n",
    "def derivative_relu(Z):\n",
    "    return 1. * (Z > 0)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z)), derivative_sigmoid(Z)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z), derivative_relu(Z)\n",
    "\n",
    "\n",
    "def softmax(Z, t = 1.0):\n",
    "    c = np.max(Z)\n",
    "    exp_a = np.exp(Z-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    result = exp_a / sum_exp_a\n",
    "    #deriv = derivative_softmax(result)\n",
    "    return result\n",
    "\n",
    "def derivative_softmax(Z):\n",
    "    # input s is softmax value of the original input x. Its shape is (1,n) \n",
    "    # e.i. s = np.array([0.3,0.7]), x = np.array([0,1])\n",
    "\n",
    "    # make the matrix whose size is n^2.\n",
    "    jacobian_m = np.diag(Z)\n",
    "\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = Z[i] * (1-Z[i])\n",
    "            else: \n",
    "                jacobian_m[i][j] = -Z[i]*Z[j]\n",
    "    return jacobian_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'derivative ReLU')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAJPCAYAAABGnGG7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa8a5dc3588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = [round(i*0.01, 5) for i in range(-1000,1000)]\n",
    "x = np.asarray(tmp)\n",
    "\n",
    "f, axarr = plt.subplots(4, sharex=True)\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(10)\n",
    "sig, derivate_sig = sigmoid(x)\n",
    "ReLU, derivate_relu = relu(x)\n",
    "axarr[0].plot(x, sig)\n",
    "axarr[0].set_title(\"Sigmoid\")\n",
    "axarr[1].plot(x, derivate_sig)\n",
    "axarr[1].set_title(\"derivative Sigmoid\")\n",
    "axarr[2].plot(x, ReLU)\n",
    "axarr[2].set_title(\"ReLU\")\n",
    "axarr[3].plot(x, derivate_relu)\n",
    "axarr[3].set_title(\"derivative ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's dimensions : (28, 28)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initialize_parameters_deep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3a39f015151f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X's dimensions : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key of parameters : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_parameters_deep' is not defined"
     ]
    }
   ],
   "source": [
    "X = train_img[0]\n",
    "print(\"X's dimensions : {}\".format(X.shape), end=\"\\n\\n\")\n",
    "\n",
    "parameters = initialize_parameters_deep([28,28,1])\n",
    "print(\"key of parameters : {}\".format(parameters.keys()), end=\"\\n\\n\")\n",
    "A_prev, W, b = X, parameters[\"W1\"], parameters[\"b1\"]\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"sigmoid\")\n",
    "print(\"With sigmoid: A[0] = \" + str(A[0]))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "print(\"With ReLU: A[0] = \" + str(A[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A[0] ** </td>\n",
    "    <td > [ 0.5         0.5         0.5         0.5         0.4987626   0.49785979\n",
    "  0.49824737  0.50023031  0.49964805  0.50018201  0.49998656  0.49860172\n",
    "  0.49866389  0.50168231  0.49806008  0.49539471  0.4938563   0.48827205\n",
    "  0.49169823  0.49441303  0.49776229  0.49857256  0.49874301  0.49950637\n",
    "  0.5         0.5         0.5         0.5       ]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A[0] ** </td>\n",
    "    <td > [ 0.          0.          0.          0.          0.          0.          0.\n",
    "  0.00092122  0.          0.00072803  0.          0.          0.\n",
    "  0.00672926  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.          0.\n",
    "  0.        ]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Cost Function\n",
    "\n",
    "지금까지 Muli Layer Forward Operation을 구현하였습니다.\n",
    "\n",
    "이번엔 Forward Operation의 결과와 label 값을 통해서 나온 결과를 비교해서 비용함수를 구현하도록 하겠습니다.\n",
    "\n",
    "**Exercise**: 여기서는 cross-entropy cost $J$를 계산합니다, 수식은 다음과 같습니다.: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL+sys.float_info.epsilon)) + np.multiply(1 - Y, np.log(1 - AL+sys.float_info.epsilon)))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_parameters_deep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6569caffd8ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize_parameters_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_parameters_deep' is not defined"
     ]
    }
   ],
   "source": [
    "classes = 10\n",
    "label = np.array([int(i==int(train_label[0])) for i in range(classes)]).reshape(classes,1)\n",
    "\n",
    "Input = train_img[0].reshape(784,1)\n",
    "\n",
    "\n",
    "X, parameters = Input, initialize_parameters_deep([784,1000,10])\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "cost = compute_cost(AL, label)\n",
    "\n",
    "#print(\"Input data shape : {}\".format(Input.shape))\n",
    "#print()\n",
    "#print(\"label : {}\".format(label))\n",
    "#print()\n",
    "#print(\"Output : {}\".format(AL))\n",
    "#print()\n",
    "print(\"cost : {}\".format(cost))\n",
    "#print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 6.881482643757871</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation module\n",
    "\n",
    "전파 알고리즘과 마찬가지로 역전파 알고리즘을 위한 함수를 구현합니다. \n",
    "\n",
    "**Reminder**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Figure 3** : Forward and Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID* <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption>\n",
    "\n",
    "<!-- \n",
    "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "Now, similar to forward propagation, you are going to build the backward propagation in three steps:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    #print(\"A_prev : {}\".format(A_prev.shape))\n",
    "    #print(\"W : {}\".format(W.shape))\n",
    "    #print(\"b : {}\".format(b.shape))\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.reshape(np.squeeze(np.sum(dZ, axis=1, keepdims=True)) / m, b.shape)\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_parameters_deep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cb71c046be76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize_parameters_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_parameters_deep' is not defined"
     ]
    }
   ],
   "source": [
    "classes = 10\n",
    "label = np.array([int(i==int(train_label[0])) for i in range(classes)]).reshape(classes,1)\n",
    "\n",
    "Input = train_img[0].reshape(784,1)\n",
    "\n",
    "\n",
    "X, parameters = Input, initialize_parameters_deep([784,1000,10])\n",
    "\n",
    "W = np.random.randn(10, 784) * 0.01\n",
    "b = np.zeros(shape=(10, 1))\n",
    "\n",
    "A, caches = linear_activation_forward(Input, W, b, \"sigmoid\")\n",
    "\n",
    "cost = compute_cost(A, label)\n",
    "\n",
    "print(\"cost : {}\".format(cost))\n",
    "\n",
    "\n",
    "# Set up some test inputs\n",
    "linear_cache, dZ = caches\n",
    "\n",
    "#print(\"dZ : {}\".format(dZ))\n",
    "#print(\"dZ type : {}\".format(dZ.dtype))\n",
    "#print()\n",
    "#print(\"cache : {}\".format(caches))\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ -1.74449981e-02]\n",
    " [  4.53713990e-03]\n",
    " [  5.22629592e-03]\n",
    " [ -6.06037379e-04]\n",
    " [  2.36638106e-03]\n",
    " [  4.78543621e-04]\n",
    " [ -1.16921660e-03]\n",
    "      .\n",
    "      .\n",
    "      .\n",
    "      </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[ 0.  0.  0. ...,  0.  0.  0.]... </td> \n",
    "    </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.24997039]... </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    "Next, you will create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n",
    "\n",
    "To help you implement `linear_activation_backward`, we provided two backward functions:\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**Exercise**: Implement the backpropagation for the *LINEAR->ACTIVATION* layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    return np.multiply(dA, activation_cache)\n",
    "\n",
    "def sigmoid_backward(dA, activation_cache):\n",
    "    #print(\"dA : {}\".format(dA))\n",
    "    #print(\"dA type : {}\".format(dA.dtype))\n",
    "    #print(\"activation_cache : {}\".format(activation_cache))\n",
    "    #print(\"activation_cache type : {}\".format(activation_cache.dtype))\n",
    "    return np.multiply(dA, activation_cache)\n",
    "\n",
    "def tanh_backward(dA, activation_cache):\n",
    "    return np.multiply(dA, activation_cache)\n",
    "\n",
    "def softmax_backward(dA, activation_cache):\n",
    "    return np.multiply(dA, activation_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    \n",
    "    # Shorten the code\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_parameters_deep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5b0f50b12428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize_parameters_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_parameters_deep' is not defined"
     ]
    }
   ],
   "source": [
    "classes = 10\n",
    "label = np.array([int(i==int(train_label[0])) for i in range(classes)]).reshape(classes,1)\n",
    "\n",
    "Input = train_img[0].reshape(784,1)\n",
    "\n",
    "\n",
    "X, parameters = Input, initialize_parameters_deep([784,1000,10])\n",
    "\n",
    "W = np.random.randn(10, 784) * 0.01\n",
    "b = np.zeros(shape=(10, 1))\n",
    "\n",
    "A, caches = linear_activation_forward(Input, W, b, \"sigmoid\")\n",
    "\n",
    "cost = compute_cost(A, label)\n",
    "\n",
    "dCost = - (np.divide(label, A) - np.divide(1 - label, 1 - A))\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dCost, caches, 'sigmoid')\n",
    "\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `update_parameters()` to update your parameters using gradient descent.\n",
    "\n",
    "**Instructions**:\n",
    "Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4e8a0838239e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"W1 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"b1 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"W2 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grads' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print (\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print (\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print (\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, \t Error : 2.3940148031264883, \t index : 3, \t label : [[0 0 0 1 0 0 0 0 0 0]]\n",
      "Iteration : 5000, \t Error : 0.07645521159505587, \t index : 2, \t label : [[0 0 1 0 0 0 0 0 0 0]]\n",
      "Iteration : 10000, \t Error : 0.0025247703723371828, \t index : 8, \t label : [[0 0 0 0 0 0 0 0 1 0]]\n",
      "Iteration : 15000, \t Error : 0.39327562613594036, \t index : 9, \t label : [[0 0 0 0 0 0 0 0 0 1]]\n",
      "Iteration : 20000, \t Error : 0.002630999692723816, \t index : 6, \t label : [[0 0 0 0 0 0 1 0 0 0]]\n",
      "Iteration : 25000, \t Error : 0.0018356821715067345, \t index : 2, \t label : [[0 0 1 0 0 0 0 0 0 0]]\n",
      "Iteration : 30000, \t Error : 0.012967292693235807, \t index : 3, \t label : [[0 0 0 1 0 0 0 0 0 0]]\n",
      "Iteration : 35000, \t Error : 0.0016975673411922586, \t index : 5, \t label : [[0 0 0 0 0 1 0 0 0 0]]\n",
      "Iteration : 40000, \t Error : 0.016354816262803728, \t index : 7, \t label : [[0 0 0 0 0 0 0 1 0 0]]\n",
      "Iteration : 45000, \t Error : 0.0001175795444339403, \t index : 3, \t label : [[0 0 0 1 0 0 0 0 0 0]]\n",
      "Iteration : 50000, \t Error : 0.0012586675433108527, \t index : 9, \t label : [[0 0 0 0 0 0 0 0 0 1]]\n",
      "Iteration : 55000, \t Error : 0.0005788461471503915, \t index : 0, \t label : [[1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Number of Train Data\n",
    "num_train_data = len(train_img)\n",
    "# Get array for Shuffle\n",
    "arr = np.arange(num_train_data)\n",
    "\n",
    "# Class number\n",
    "classes = 10\n",
    "\n",
    "# Define Hyper parameters\n",
    "epoch = 1\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "\n",
    "# Initilizer Weights and bias\n",
    "W1 = np.random.randn(1000, 784) * 0.1\n",
    "b1 = np.zeros(shape=(1000, 1))\n",
    "\n",
    "W2 = np.random.randn(100, 1000) * 0.1\n",
    "b2 = np.zeros(shape=(100, 1))\n",
    "\n",
    "W3 = np.random.randn(10, 100) * 0.1\n",
    "b3 = np.zeros(shape=(10, 1))\n",
    "\n",
    "prameters = {}\n",
    "parameters[\"W1\"] = W1\n",
    "parameters[\"b1\"] = b1\n",
    "parameters[\"W2\"] = W2\n",
    "parameters[\"b2\"] = b2\n",
    "parameters[\"W3\"] = W3\n",
    "parameters[\"b3\"] = b3\n",
    "\n",
    "grads = {}\n",
    "Error = []\n",
    "for j in range(epoch):\n",
    "    \n",
    "    # Shuffle Data\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "    for i in range(num_train_data):\n",
    "        \n",
    "        # Define Input Data Pair\n",
    "        data_index = arr[i]\n",
    "        X, Y = train_img[data_index].reshape(784,1), train_label[data_index]\n",
    "        label = np.array([int(i==int(Y)) for i in range(classes)]).reshape(classes,1)\n",
    "\n",
    "        # Forward Operation\n",
    "        A1, caches1 = linear_activation_forward(X, parameters[\"W1\"], parameters[\"b1\"], \"sigmoid\")\n",
    "        A2, caches2 = linear_activation_forward(A1, parameters[\"W2\"], parameters[\"b2\"], \"sigmoid\")\n",
    "        A3, caches3 = linear_activation_forward(A2, parameters[\"W3\"], parameters[\"b3\"], \"sigmoid\")\n",
    "        \n",
    "        # MSE Error\n",
    "        m = label.shape[1]\n",
    "        cost = (1/m) * np.sum(np.square(label - A3))\n",
    "        \n",
    "        # Error Gradient\n",
    "        dCost = (-2/m)*(label-A3)\n",
    "    \n",
    "        # Display Error\n",
    "        if(i % 5000 == 0):\n",
    "            print(\"Iteration : {}, \\t Error : {}, \\t index : {}, \\t label : {}\".format((j*num_train_data)+i,cost, Y, label.reshape(1,10)))\n",
    "            Error.append(cost)\n",
    "            \n",
    "        # Backpropagation\n",
    "        grads[\"dA3\"], grads[\"dW3\"], grads[\"db3\"] = linear_activation_backward(dCost, caches3, 'softmax')\n",
    "        grads[\"dA2\"], grads[\"dW2\"], grads[\"db2\"] = linear_activation_backward(grads[\"dA3\"], caches2, 'sigmoid')\n",
    "        grads[\"dA1\"], grads[\"dW1\"], grads[\"db1\"] = linear_activation_backward(grads[\"dA2\"], caches1, 'sigmoid')\n",
    "\n",
    "        # Update Parameters\n",
    "        parameters = update_parameters(parameters, grads, 0.1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAARiCAYAAADVzyypAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa8a8dbbac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(Error)\n",
    "plt.plot(x, Error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGQAAARiCAYAAADiLSHYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3V2oZfV9xvHfT0+L+G5SlCEOtBQp\nCJIoQ2iohEiqSL1IvLAoIl4ET5FGWtJcSANpLkPoy2XJSCRetJZKW5ILbasiiFBqYokvcbSGIaXK\nZEQimDJEE/33ojtlIrPmjOecef4zez4fGM5+WWev5074utbZPcYoAAAAAHLOmj0AAAAA4EwjyAAA\nAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACE\nbSRP1t0jeT4AAACAtDFGb3WMK2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAA\ngDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgLAd\nBZnuvrG7X+7uH3T3vbs1CgAAAGCd9Rhje7/YfXZV/WdVXV9Vr1bVd6rqtjHGi8f5ne2dDAAAAOA0\nMcborY7ZyRUyH6+qH4wxDo4x3qmqv6uqz+zg8wAAAADOCDsJMh+pqv8+6vmrq9cAAAAAOI6Nk32C\n7t6sqs2TfR4AAACA08VOgsxrVbX3qOeXr177JWOM/VW1v8rfkAEAAACo2tktS9+pqiu6+ze6+1er\n6taq+vbuzAIAAABYX9u+QmaM8fPu/nxV/UtVnV1V948xvr9rywAAAADW1La/9npbJ3PLEgAAALDm\nTvbXXgMAAACwDYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACE\nCTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIA\nAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAA\nYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIM\nAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAA\nQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJgg\nAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAA\nABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAm\nyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAA\nAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACE\nCTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIA\nAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAA\nYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIM\nAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAA\nQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJgg\nAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAA\nABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAm\nyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAA\nAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACE\nCTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIA\nAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAA\nYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYYIMAAAAQJggAwAAABAmyAAAAACECTIAAAAAYRs7\n+eXu/mFV/aSq3q2qn48x9u3GKAAAAIB1tqMgs3LdGOONXfgcAAAAgDOCW5YAAAAAwnYaZEZV/Wt3\nP9Pdm7sxCAAAAGDd7fSWpWvHGK9196VV9Wh3vzTGePLoA1ahRqwBAAAAWOkxxu58UPdXqup/xhh/\nfpxjdudkAAAAAKeoMUZvdcy2b1nq7vO6+4JfPK6qG6rqhe1+HgAAAMCZYie3LF1WVf/U3b/4nL8d\nY/zzrqwCAAAAWGO7dsvSCZ3MLUsAAADAmjuptywBAAAAsD2CDAAAAECYIAMAAAAQJsgAAAAAhAky\nAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAA\nAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGC\nDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAA\nAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECY\nIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMA\nAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGEbswcAp47Nzc3ZExbd\nc889sycsOnz48OwJi44cOTJ7wqL9+/fPnnBMBw8enD1h0Ysvvjh7AgAAu8QVMgAAAABhggwAAABA\nmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCAD\nAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAA\nECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbI\nAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAA\nAIQJMgAAAABhggwAAABAWI8xcifrzp0M+MDefPPN2RMWXXTRRbMncIZ45513Zk9Y9Nprr82eALDr\nDh8+PHvCoi996UuzJyx64oknZk8AjmOM0Vsd4woZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZ\nAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAA\ngDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBB\nBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAA\nACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACCs\nxxi5k3XnTgZ8YLfccsvsCYuuueaa2RMWPf/887MnLLrqqqtmT1j0iU98YvaEY7r66qtnT1h0wQUX\nzJ6w6K233po9YdGFF144ewK77L333ps94ZiOHDkye8Ki888/f/aE09KDDz44e8Ki22+/ffYE4DjG\nGL3VMa6QAQAAAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAgTZAAAAADCBBkAAACAMEEGAAAA\nIEyQAQAAAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQ\nAQAAAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAA\nAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAgT\nZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAjbmD0AOHU89NBDsycsOpW3sV4+/OEPz56w6Lrrrps9\nYdFjjz02e8Ki66+/fvYEdtmRI0dmTzimZ555ZvaERQcPHpw9YdE555wze8Kil19+efYEYI25QgYA\nAAAgTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAg\nTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACBNkAAAAAMK2DDLdfX93v97dLxz12oe6\n+9HufmX185KTOxMAAABgfZzIFTLfrKob3/favVX1+Bjjiqp6fPUcAAAAgBOwZZAZYzxZVT9+38uf\nqaoHVo8fqKrP7vIuAAAAgLW13b8hc9kY49Dq8Y+q6rJd2gMAAACw9jZ2+gFjjNHdY+n97t6sqs2d\nngcAAABgXWz3CpnD3b2nqmr18/WlA8cY+8cY+8YY+7Z5LgAAAIC1st0g8+2qunP1+M6q+tbuzAEA\nAABYfyfytdcPVtW/VdVvdfer3f25qvpqVV3f3a9U1e+ungMAAABwArb8GzJjjNsW3vr0Lm8BAAAA\nOCNs95YlAAAAALZJkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAA\ngDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAI6zFG7mTduZMBAEDQXXfd\nNXvCoq9//euzJyw6dOjQ7AmLPvrRj86esOiNN96YPQE4jjFGb3WMK2QAAAAAwgQZAAAAgDBBBgAA\nACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBM\nkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEA\nAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAI\nE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QA\nAAAAwgQZAAAAgLAeY+RO1p07GQAAa2nPnj2zJxzTK6+8MnvCovPOO2/2hEWbm5uzJyy67777Zk8A\nTlNjjN7qGFfIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCAD\nAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAA\nECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbI\nAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAA\nAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhG7MHAADAB/HlL3959oRjOvfcc2dP\nWPTTn/509oRFzz777OwJAFO4QgYAAAAgTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAA\nCBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACBNk\nAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACBNkAAAA\nAMIEGQAAAIAwQQYAAAAgTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACBNkAAAAAMIE\nGQAAAIAwQQYAAAAgTJABAAAACBNkAAAAAMIEGQAAAIAwQQYAAAAgTJABAAAACNuYPQAAgFPPTTfd\nNHvCorvuumv2hNPOrbfeOnvCoqeffnr2BIApXCEDAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCAD\nAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAA\nECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbI\nAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAA\nAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIRt\nzB4AAMCp5+abb549YdFZZ52a/0/xwIEDsycsevjhh2dPAOB9Ts3/mgEAAACsMUEGAAAAIEyQAQAA\nAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAgTZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAgT\nZAAAAADCBBkAAACAMEEGAAAAIEyQAQAAAAjbMsh09/3d/Xp3v3DUa1/p7te6+3urf793cmcCAAAA\nrI8TuULmm1V14zFe/6sxxsdW/x7e3VkAAAAA62vLIDPGeLKqfhzYAgAAAHBG2MnfkPl8dz+3uqXp\nkqWDunuzu7/b3d/dwbkAAAAA1sZ2g8xfV9VvVtXHqupQVf3F0oFjjP1jjH1jjH3bPBcAAADAWtlW\nkBljHB5jvDvGeK+q7quqj+/uLAAAAID1ta0g0917jnp6c1W9sHQsAAAAAL9sY6sDuvvBqvpUVf1a\nd79aVX9WVZ/q7o9V1aiqH1bVH5zEjQAAAABrZcsgM8a47Rgvf+MkbAEAAAA4I+zkW5YAAAAA2AZB\nBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAA\nACBMkAEAAAAIE2QAAAAAwgQZAAAAgDBBBgAAACBsY/YAAIAz1bnnnjt7wqIbbrhh9oRF77777uwJ\nx/TFL35x9oRFP/vZz2ZPAOB9XCEDAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAA\nAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJ\nMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAA\nAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABh\nggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAELYxewAAwJnqa1/72uwJiy6/\n/PLZExY999xzsycc0yOPPDJ7AgCnEVfIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJ\nMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAA\nAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABh\nggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwA\nAABAmCADAAAAECbIAAAAAIQJMgAAAABhggwAAABAmCADAAAAECbIAAAAAIQJMgAAAABhG7MHAACc\nTHfcccfsCYvuvvvu2RMWvf3227MnLLr33ntnTwCAHXOFDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAA\nAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGC\nDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAA\nAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECY\nIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMA\nAAAQ1mOM3Mm6cycDAGIuvfTS2RMWvfTSS7MnLLr44otnT1j01FNPzZ6w6JOf/OTsCQBwXGOM3uoY\nV8gAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgA\nAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAA\nhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAky\nAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAA\nAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGE9xsidrDt3MgBYM2efffbsCYsOHjw4e8KivXv3\nzp6w6M0335w9YdG11147e8KiAwcOzJ4AAMc1xuitjnGFDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAA\nAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGC\nDAAAAECYIAMAAAAQJsgAAAAAhG0ZZLp7b3c/0d0vdvf3u/uPVq9/qLsf7e5XVj8vOflzAQAAAE5/\nJ3KFzM+r6k/GGFdW1W9X1R9295VVdW9VPT7GuKKqHl89BwAAAGALWwaZMcahMcZ/rB7/pKoOVNVH\nquozVfXA6rAHquqzJ2skAAAAwDr5QH9Dprt/vaqurqp/r6rLxhiHVm/9qKou29VlAAAAAGtq40QP\n7O7zq+ofquqPxxhvdff/vzfGGN09Fn5vs6o2dzoUAAAAYF2c0BUy3f0r9X8x5m/GGP+4evlwd+9Z\nvb+nql4/1u+OMfaPMfaNMfbtxmAAAACA092JfMtSV9U3qurAGOMvj3rr21V15+rxnVX1rd2fBwAA\nALB+TuSWpd+pqjuq6vnu/t7qtT+tqq9W1d939+eq6r+q6vdPzkQAAACA9bJlkBljPFVVvfD2p3d3\nDgAAAMD6+0DfsgQAAADAzgkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECY\nIAMAAAAQJsgAAAAAhAkyAAAAAGGCDAAAAECYIAMAAAAQJsgAAAAAhAkyAAAAAGEbswcAACfmyiuv\nnD1h0d69e2dPOC194QtfmD1h0YEDB2ZPAIC15goZAAAAgDBBBgAAACBMkAEAAAAIE2QAAAAAwgQZ\nAAAA+N/27iDU0rO+4/jvj2M2aiBFkJDG2g7ddTGW4KZSZtOiLmLdSAMpdqWLCgpdtHHTbAolqO1O\nUAwY0JaCtnVRSF0ITTdiEoKJCW2lRGoyTRgMqKtS83RxjzDYuZORzv29897z+cBwz33POff8F/Pw\n3Pvlfc+BMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkA\nAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACA\nMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEG\nAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAA\noOzC1gMAwO3k4sWLW49wqieeeGLrEXbpkUce2XqEUz322GNbjwAAbMQZMgAAAABlggwAAABAmSAD\nAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAA\nUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbI\nAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAA\nAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJ\nMgAAAABlggwAAABA2YWtBwCA28lDDz209QinuvPOO7ceYZcef/zxrUc41Vpr6xEAgI04QwYAAACg\nTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJAB\nAAAAKBNkAAAAAMoEGQAAAIBaAIiaAAAJs0lEQVQyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAy\nQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYA\nAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACg\nTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACg7MLWAwBwnO6///6tR7iuBx98cOsRAAA4As6QAQAA\nACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgT\nZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAA\nAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADK\nBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkA\nAACAMkEGAAAAoEyQAQAAACgTZAAAAADKLmw9AADH6fLly1uPcF133HHH1iPs0muvvbb1CKe6nWcD\nAI6XM2QAAAAAygQZAAAAgDJBBgAAAKBMkAEAAAAoE2QAAAAAygQZAAAAgDJBBgAAAKBMkAEAAAAo\nE2QAAAAAygQZAAAAgDJBBgAAAKBMkAEAAAAoE2QAAAAAygQZAAAAgDJBBgAAAKDsDYPMzNw7M9+c\nmedn5rsz84nD8Ydn5qWZeebw7wNnPy4AAADA/l24icf8T5I/Xms9PTNvS/LUzHzjcN9frrU+fXbj\nAQAAAJw/bxhk1lpXklw53P7xzLyQ5J6zHgwAAADgvPqF3kNmZt6V5N1JvnU49PGZ+c7MPDozd93i\n2QAAAADOpZsOMjPz1iRfTfLJtdaPknwuycUkl3JyBs1nTnneR2fmyZl58hbMCwAAALB7NxVkZubN\nOYkxX15rfS1J1lqvrLV+utZ6PckXkrznes9da31+rXXfWuu+WzU0AAAAwJ7dzKcsTZIvJnlhrfXZ\na47ffc3DPpTkuVs/HgAAAMD5czOfsvRbSf4gybMz88zh2KeSPDAzl5KsJC8m+diZTAgAAABwztzM\npyz9S5K5zl3/eOvHAQAAADj/fqFPWQIAAADg/0+QAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQ\nAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAAACgTZAAAAADKBBkAAACAMkEGAAAAoEyQAQAA\nACi7sPUAAMDNefnll7ce4VSXLl3aeoRTXb16desRAAD+D2fIAAAAAJQJMgAAAABlggwAAABAmSAD\nAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAA\nUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbI\nAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAA\nAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJMgAAAABlggwAAABAmSADAAAAUCbIAAAAAJQJ\nMgAAAABls9bqvdhM78UAAAAANrDWmjd6jDNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAA\nAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoE\nGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAA\nAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAy\nQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAICyC+XX\nu5rk+7foZ7398PPg2FkLYB1AYh1AYh1AYh3cDn7lZh40a62zHuRMzMyTa637tp4DtmYtgHUAiXUA\niXUAiXWwJy5ZAgAAACgTZAAAAADK9hxkPr/1AHCbsBbAOoDEOoDEOoDEOtiN3b6HDAAAAMBe7fkM\nGQAAAIBd2mWQmZn3zcy/zsz3ZuZPt54HtjAzL87MszPzzMw8ufU80DAzj87MqzPz3DXHfmlmvjEz\n/374eteWM0LDKWvh4Zl56bAvPDMzH9hyRjhLM3PvzHxzZp6fme/OzCcOx+0JHI0brAP7wU7s7pKl\nmXlTkn9L8jtJfpDk20keWGs9v+lgUDYzLya5b611detZoGVmfjvJT5I8ttb6jcOxR5L8cK31F4dI\nf9da60+2nBPO2ilr4eEkP1lrfXrL2aBhZu5Ocvda6+mZeVuSp5L8XpI/jD2BI3GDdfDh2A92YY9n\nyLwnyffWWv+x1vrvJH+T5IMbzwRAwVrrn5P88OcOfzDJlw63v5STX0TgXDtlLcDRWGtdWWs9fbj9\n4yQvJLkn9gSOyA3WATuxxyBzT5L/vOb7H8R/Oo7TSvJPM/PUzHx062FgQ+9Ya1053P6vJO/YchjY\n2Mdn5juHS5pcqsFRmJl3JXl3km/FnsCR+rl1kNgPdmGPQQY48d611m8meX+SPzqcvg5HbZ1ch7uv\na3Hh1vlckotJLiW5kuQz244DZ29m3prkq0k+udb60bX32RM4FtdZB/aDndhjkHkpyb3XfP/Lh2Nw\nVNZaLx2+vprk73JyOR8co1cO11D/7FrqVzeeBzax1nplrfXTtdbrSb4Q+wLn3My8OSd/hH55rfW1\nw2F7AkfleuvAfrAfewwy307y6zPzqzNzR5LfT/L1jWeCqpl5y+GNuzIzb0nyu0meu/Gz4Nz6epKP\nHG5/JMk/bDgLbOZnf4QefCj2Bc6xmZkkX0zywlrrs9fcZU/gaJy2DuwH+7G7T1lKksPHdv1Vkjcl\neXSt9ecbjwRVM/NrOTkrJkkuJPmKdcAxmJm/TnI5yduTvJLkz5L8fZK/TfLOJN9P8uG1ljc75Vw7\nZS1czsnp6SvJi0k+ds17acC5MjPvTfJEkmeTvH44/KmcvH+GPYGjcIN18EDsB7uwyyADAAAAsGd7\nvGQJAAAAYNcEGQAAAIAyQQYAAACgTJABAAAAKBNkAAAAAMoEGQAAAIAyQQYAAACgTJABAAAAKPtf\n/jXL93ksNwAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa8a8b09b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict value : 7, OutPut scalar : \n",
      "\n",
      "[[  7.56795827e-05   8.61170014e-05   3.78962467e-04   1.72624322e-02\n",
      "    1.24757830e-05   3.63087738e-05   5.80795102e-07   9.99516286e-01\n",
      "    4.83606702e-05   4.90498101e-04]]\n"
     ]
    }
   ],
   "source": [
    "classes = 10\n",
    "label = np.array([int(i==int(val_label[0])) for i in range(classes)]).reshape(classes,1)\n",
    "Input = val_img[0]\n",
    "plt.imshow(Input, cmap='Greys_r')\n",
    "plt.show()\n",
    "\n",
    "Input = Input.reshape(784,1)\n",
    "\n",
    "\n",
    "A1, caches1 = linear_activation_forward(Input, parameters[\"W1\"], parameters[\"b1\"], \"sigmoid\")\n",
    "A2, caches2 = linear_activation_forward(A1, parameters[\"W2\"], parameters[\"b2\"], \"sigmoid\")\n",
    "A3, caches3 = linear_activation_forward(A2, parameters[\"W3\"], parameters[\"b3\"], \"sigmoid\")\n",
    "\n",
    "val = np.argmax(A3)\n",
    "print(\"Predict value : {}, OutPut scalar : \\n\\n{}\".format(val, A3.reshape(1,10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
