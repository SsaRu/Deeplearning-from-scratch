{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<br/>\n",
    "\n",
    "해당 스크립트에서는 Multi Layer Perceptron이라고 불리는 기본적인 인공 신경망을 Python 라이브러리인 numpy를 이용하여 구현한 스크립트입니다.\n",
    "\n",
    "해당 스크립트는 다음과 같으신 분을 위해서 쓰여졌습니다.\n",
    "\n",
    "1. Python 기초를 아시는 분\n",
    "2. 기본적인 딥러닝의 이론에 대해서 이해가 있으신분. \n",
    "3. Tensorflow나 Torch등 기본적인 프레임워크로도 구현을 해봤는데, 내부 동작은 어떻게 돌아가는지에 대해서 궁금하신 분\n",
    "\n",
    "<br/>\n",
    "만약에 해당 스크립트가 너무 어렵게 느껴지시거나 이론을 조금 더 알고 싶으시다면 아래의 링크에서 딥러닝의 기초에 대해서 배울 수 있으니, 먼저 기본적인 딥러닝에 대해서 학습하고 오시기를 권장해드립니다\n",
    "\n",
    "<br/>\n",
    "[<font size=\"3\">[1]. 모두의 딥러닝 (무료/국문)](https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)\n",
    "<br/><br/>\n",
    "[<font size=\"3\">[2]. 헬로 딥러닝 (무료/국문)](https://www.youtube.com/watch?v=yWySw4EfSJc&t=424s)\n",
    "<br/><br/>\n",
    "[<font size=\"3\">[3]. 코세라, Deep Learning Specialization (유료/영문)](https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&campaignid=917423980&adgroupid=46295378339&device=c&keyword=coursera%20deep%20learning&matchtype=p&network=g&devicemodel=&adpostion=1t1&creativeid=217989182387&hide_mobile_promo&gclid=EAIaIQobChMI_cnop7HU1wIVzgMqCh2CdAQiEAAYASAAEgKbcfD_BwE)\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Neural Network :: Multi Layer Perceptron\n",
    "\n",
    "\n",
    "<br/>\n",
    "시작하기 앞서서 해당 스크립트의 핵심은 다음과 같이 구현되어 있습니다.\n",
    "\n",
    "- 기본적인 인공신경망(Multi Layer Perceptron)에 대해서 설명합니다.\n",
    "\n",
    "\n",
    "- <전파 알고리즘(Forward Operation)> / 역전파 알고리즘(BackPropagation) / 활성화함수(Activation function) / 분류기(Softamx) / 에러함수(MSE, Cross-Entropy Error) 에 대한 작은 모듈들을 만듭니다.\n",
    "\n",
    "\n",
    "- 해당 모듈을 만들고나면, 이를 연결하여 하나의 인공신경망을 만듭니다.\n",
    "\n",
    "\n",
    "- 이에 대한 결과를 확인합니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "**<font size=\"5\">목차**\n",
    "\n",
    "**<font size=\"3\">1. MNIST DataSet 설명 및 데이터 로드**<br/>\n",
    "\n",
    "**<font size=\"3\">2. Multi-Layer Perceptron Overview**\n",
    "    \n",
    "**<font size=\"3\">3. 전파 알고리즘(Forward Operation) 개념 :: 기초**\n",
    "    \n",
    "**<font size=\"3\">4. 전파 알고리즘(Forward Operation) 개념 :: 행렬**\n",
    "\n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">5. TODO 활성화 함수(Activation Function) 개념**\n",
    "\n",
    "**<font size=\"3\">6. 비용 함수(Cost Function) 개념 :: 분류 오차/MSE/CEE**\n",
    "\n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">7. TODO 경사 하강 알고리즘(Gradient Descent) 개념 **\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">8. TODO 역 전파 알고리즘(Backpropagation) 개념 **\n",
    "    \n",
    "**<font size=\"3\">9. 가중치(weights) 생성 및 초기화 :: 단일 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">10. 가중치(weights) 생성 및 초기화 :: 다중 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">11. 전파 알고리즘(Forward Operation) :: 단일 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">12. 활성화 함수(Activation Function)와 활성화 함수의 미분 (Code)**\n",
    "\n",
    "**<font size=\"3\">13. 다중 전파 알고리즘(Forward Operation) :: 다중 layer (Code)**\n",
    "\n",
    "**<font size=\"3\">14. 비용 함수(Cost Function) (Code)**\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">15. TODO 경사 하강 알고리즘(Gradient Descent) (Code)**\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">16. TODO 역 전파 알고리즘(Backpropagation) (Code)**\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">17. TODO MNIST 학습 :: numpy 기반 (Code)**\n",
    "    \n",
    "**<font color=\"#BDBDBD\"><font size=\"3\">17. TODO MNIST 학습 :: tensorflow 기반 (Code)**\n",
    "\n",
    "<br/>\n",
    "<br/>*<font color=\"red\">해당 스크립트는 코세라의 Deep Learning Specialization의 코드를 기반으로 했음을 알려드립니다*\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - MNIST Data Load \n",
    "\n",
    "먼저, 예제 데이터를 입력으로 사용하기 위해서 MNIST 데이터를 로드하겠습니다.\n",
    "\n",
    "MNIST 데이터는 Figure 1과 같이 사람들의 다양한 손글씨 이미지를 대량으로 모아놓은 데이터입니다.\n",
    "<br/><br/>\n",
    "<img src=\"images/14481-mnist-dataset.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 1</u>: MNIST DataSet </center></caption>\n",
    "<br/><br/>\n",
    "\n",
    "일반적으로 모든 이미지는 데이터로 변환할 수 있는데, 아래의 Figure 2와 같이 하얀색에 가까울 수록 값이 0에 가까워지고\n",
    "검은색에 가까워질 수록 값이 1에 가까워지는 형태로 표현할 수 있습니다.\n",
    "<br/><br/>\n",
    "그림과 같이 MNIST 데이터는 다양한 손글씨 데이터를 width=28, height=28의 흑백이미지(검은색과 하얀색으로 이루어진)의 데이터 집합이라고 보시면 됩니다.\n",
    "\n",
    "<br/><br/>\n",
    "*<font color=\"red\">이해를 위해서 그림과 일치하게 설명한 것일 뿐입니다. 실제로는 하얀색에 가까울 수록 값은 \"1\", 검은색에 가까울 수록 값은 \"0\"에 근접합니다.*<br/><br/>\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/d4e5709ebb4ba940126de44c76ca71b0.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 2</u>: Image Data </center></caption>\n",
    "\n",
    "\n",
    "MNIST 데이터의 특징은 다음과 같습니다.\n",
    "\n",
    "- 이미지의 width와 height가 모두 28입니다.\n",
    "\n",
    "\n",
    "- 흑백 이미지입니다.\n",
    "\n",
    "\n",
    "- 따라서 MNIST 데이터의 갯수는 $28x28x1=784$이 됩니다.<br/>\n",
    "*<font color=\"red\">이 또한 이해를 위해서 간략하게 설명한 것입니다. 실은 데이터의 차원이라고 말하는 게 더 옳은 표현입니다.*<br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - 1 MNIST Data Load \n",
    "\n",
    "<br/>\n",
    "**Note**: 일반적으로 이미지 데이터를 읽어들이게되면 numpy의 dtype(\"uint8\")으로 저장이 됩니다. uint8은 0~255 사이의 값으로 표현되며, 이를 \"0\"과 \"1\"사이의 데이터로 정규화하기 위해서 255로 나누어주는 과정을 거칩니다.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Data Load\n",
    "def read_data(label_path, image_path):\n",
    "    with gzip.open(label_path) as flbl:\n",
    "        _, _ = struct.unpack(\">II\", flbl.read(8))\n",
    "        label = np.fromstring(flbl.read(), dtype=np.int8)\n",
    "    with gzip.open(image_path, 'rb') as fimg:\n",
    "        _, _, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n",
    "    return (label, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "(train_label, train_img) = read_data(path+'train-labels-idx1-ubyte.gz', path+'train-images-idx3-ubyte.gz')\n",
    "(val_label, val_img) = read_data(path+'t10k-labels-idx1-ubyte.gz', path+'t10k-images-idx3-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAACBCAYAAABXearSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncveO5Y1kGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc92e52f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(train_img[i], cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "train_label = train_label.astype(np.int64)\n",
    "train_img = train_img.astype(np.float32) / 255\n",
    "val_label = val_label.astype(np.int64)\n",
    "val_img = val_img.astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## 2 - Multi Layer Perceptron :: Overview\n",
    "\n",
    "MLP(Multi Layer Perceptron :: 이제부터 편의상 MLP라고 줄여서 이야기하도록 하겠습니다)의 순서는 다음과 같습니다.\n",
    "\n",
    "1.실행 :: 학습이 완료되었을 경우\n",
    "\n",
    "\n",
    "- 데이터 입력 \n",
    "- 전파 알고리즘 (Forward Operation)\n",
    "<br/>\n",
    "\n",
    "<img src=\"images/MLP_test.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> <u>Figure 3</u>: MLP Test</center></caption>\n",
    "\n",
    "\n",
    "2.학습 \n",
    "\n",
    "\n",
    "- 데이터 입력\n",
    "- 전파 알고리즘 (Forward Operation)\n",
    "- 비용 함수 계산 (Cost Calculation)\n",
    "- 역전파 알고리즘 (Backpropagation)\n",
    "- 위의 4가지 순서를 반복 (Iteration)\n",
    "\n",
    "<img src=\"images/MLP_train.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> <u>Figure 4</u>: MLP Train</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "Figure 5는 그림에서 위의 알고리즘이 어디에 해당하는지 표시한 그림입니다.\n",
    "\n",
    "<img src=\"images/MLP_train_detail.png\" style=\"width:850px;height:400px;\">\n",
    "<caption><center> <u>Figure 5</u>: MLP Train Detail</center></caption>\n",
    "\n",
    "### 2-1 전파 알고리즘(Forward Operation) :: 기본 컨셉 (basic concept)\n",
    "\n",
    "<br/>\n",
    "이제 이 MLP에서 한개의 레이어를 자세히 살펴보도록 하겠습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"images/Multilayer-Perceptron-Forward-Propagation.png\" style=\"width:650px;height:550px;\">\n",
    "<caption><center> <u>Figure 6</u>: 1-Layer MLP </center></caption>\n",
    "\n",
    "\n",
    "**<font size=\"3\"><font color=\"red\">Note**: <br/><br/>\n",
    "*$l1,l2$* : 각각 하나의 입력데이터를 의미합니다.\n",
    "\n",
    "*화살표의 숫자* : 가중치 값을 의미합니다. 해당 스크립트에서는 $l1$에서 $H1$으로 가는 가중치를 $W_{l_{1}}^{H_{1}}$이라고 하겠습니다\n",
    "\n",
    "**$H1, H2, H3$** : 각각의 뉴런(Hidden Unit)입니다.\n",
    "\n",
    "**$O1, O2$** : 각각의 출력 뉴런을 의미합니다\n",
    "\n",
    "*활성화 함수(Activation function)* : 해당 그림에는 표기되어있지 않으나, 활성화 함수는 `Sigmoid`입니다.<br/>\n",
    "`Sigmoid`함수는 추후에 설명하겠으나, 흐름을 따라가기 위해서 기본적인 수식이 이렇다는 것만 알아두고 넘어가겠습니다.<br/>\n",
    "\n",
    "$$Sigmoid = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "이제부터 전파 알고리즘이 어떻게 진행되는지, 차례차레 풀어보도록 하겠습니다.\n",
    "\n",
    "기본적으로 인공 신경망의 연산은 다음과 같은 수식을 띕니다.\n",
    "\n",
    "<br/>\n",
    "$$Y = \\sigma(WX+b)$$\n",
    "\n",
    "<br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "\n",
    "수식이 어렵게 느껴지실 수도 있지만, 정말 간단합니다.\n",
    "\n",
    "$W$ : 가중치를 의미합니다.\n",
    "\n",
    "$X$ : 입력값을 의미합니다.\n",
    "\n",
    "$b$ : 편향(bias)라고 불리는 숫자(상수)입니다.\n",
    "\n",
    "$\\sigma$ : 활성화 함수(Acitvation function)을 의미합니다\n",
    "\n",
    "$Y$ : 출력값입니다.\n",
    "\n",
    "이를 가지고 말로 풀어서 써보자면, \n",
    "- 입력과 가중치를 곱합니다.\n",
    "- 그 결과에 어떠한 숫자(편향)를 더합니다.\n",
    "- 그 결과를 활성화 함수에 넣어줍니다.\n",
    "- 활성화 함수의 결과가 출력값이 됩니다.\n",
    "\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "**<font size=\"4\">1. $H1, H2, H3$ 뉴런 Input값과 Output값 유도**\n",
    "<br/><br/>\n",
    "그럼 이제 $H1$의 결과가 어떻게 나오게 됬는지 $Y = \\sigma(WX+b)$을 통해서 확인해보겠습니다.\n",
    "    \n",
    "\n",
    "- $Z = (l1 \\times 0.3) + (l2 \\times 0.2) + 0 = (10\\times0.3) + (20\\times0.2) + 0 = 7$ :: 여기에서 bias는 없으므로 0으로 대체합니다.\n",
    "\n",
    "\n",
    "- $Sigmoid(Z) = \\frac{1}{1+e^{-7}} = 0.999$\n",
    "<br/><br/>\n",
    "\n",
    "이런 형태로 $H2, H3$ 그리고 더 나아가서 $O1, O2$값도 똑같은 방법으로 구할 수 있습니다.\n",
    "\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Forward Operation :: matrix concept\n",
    "<br/>\n",
    "지금까지 기본적인 $Y = \\sigma(WX+b)$를 이용해서, 어떻게 MLP의 전파 알고리즘이 진행되는지 확인하였습니다\n",
    "\n",
    "이번에는 이를 모두 하나의 행렬로 묶어서 표현하고 연산하는 방법에 대해서 소개하겠습니다.\n",
    "\n",
    "Figure 6의 값들은 모두 행렬로 표현할 수 있고, 행렬 연산으로 수행할 수 있습니다.\n",
    "\n",
    "먼저 입력에 대해서 먼저 보면 $l1 = 10,  l2 = 20$ 입니다. \n",
    "\n",
    "<br/>\n",
    "이를 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$Input = \\begin{bmatrix}\n",
    "    10 & 20\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "Input layer와 Hidden Layer 사이의 가중치들을 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$W_{input}^{hidden} = \\begin{bmatrix}\n",
    "    0.3  & -0.1  &  1.1 \\\\\n",
    "    0.2  & -0.2  &  -0.5\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "<br/><br/>\n",
    "Hidden Layer와 Output layer 사이의 가중치들을 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    "$W_{hidden}^{out} = \\begin{bmatrix}\n",
    "    1.1  &  0.4 \\\\\n",
    "    0.5  &  0.3 \\\\\n",
    "    0.7  &  0.2\n",
    "\\end{bmatrix}\\;\\;\\; $\n",
    "\n",
    "이렇게 데이터들을 행렬로 표현하고, 위와 똑같이 $Y = \\sigma(WX+b)$를 적용하면 더 간편하게 표현할 수 있습니다.\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\"><font color=\"red\">Note**<br/>\n",
    "    \n",
    "여기서의 곱셈 연산은 행렬곱셈 연산을 의미합니다.<br/>\n",
    "행렬 곱셈 연산은 다음 Figure 7와 같은 방식으로 이루어집니다.\n",
    "<img src=\"images/animation03.gif\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 7</u>: Matrix Multiply</center></caption>\n",
    "\n",
    "\n",
    "    \n",
    "<br/><br/>\n",
    "\n",
    "**<font size=\"4\">1. $H1, H2, H3$ 뉴런값들을 행렬 연산으로 유도** <br/>\n",
    "    \n",
    "계속 이야기했듯이 행렬표현으로 변경해도 전파 알고리즘 수식인 $Y = \\sigma(WX+b)$가 똑같이 적용된다고 말씀드렸었습니다.\n",
    "\n",
    "그럼 지금부터 행렬 연산을 기반으로 $H1, H2, H3$ 뉴런값을 한번에 구해보도록 하겠습니다.\n",
    "\n",
    "\n",
    "- $Z=WX+b$<br/><br/>\n",
    "$Z_{hidden} = \\begin{bmatrix}H1&H2&H3\\end{bmatrix} = \\begin{bmatrix}10&20\\end{bmatrix}\\times\\begin{bmatrix}0.3&-0.1&1.1\\\\0.2&-0.2&-0.5\\end{bmatrix} + \\begin{bmatrix}0&0&0\\end{bmatrix} = \\begin{bmatrix}7&-5&1\\end{bmatrix}$<br/><br/><br/>\n",
    "\n",
    "- $Y_{hidden} = Sigmoid(Z_{hidden})$<br/><br/>\n",
    "$ Y= \\begin{bmatrix}Sigmoid(H1)&Sigmoid(H2)&Sigmoid(H3)\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{1+e^{-7}}&\\frac{1}{1+e^{5}}&\\frac{1}{1+e^{-1}}\\end{bmatrix} = \\begin{bmatrix}0.999&0.007&0.731\\end{bmatrix}$<br/>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "이런 방식으로 똑같이 $O1, O2$도 구할 수 있습니다.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Cost Function\n",
    "<br/><br/>\n",
    "지금까지 MLP의 전파 알고리즘(Forward Operation)의 개념을 확인하였습니다. \n",
    "\n",
    "이렇게 MLP의 전파 알고리즘을 타고 최종적으로 출력된 값은 중요한 의미를 갖습니다.\n",
    "\n",
    "예를 들어 어떠한 이미지를 입력데이터로 인공신경망에게 넣었을 때, 출력값은 이게 **개**인지 **고양이**인지에 대한 출력으로 볼 수 있습니다.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "만약에 우리가 가중치값들을 임의로 준 상태에서 입력에 고양이 이미지를 넣는다면 \n",
    "\n",
    "출력값은 분명 정확히 이게 개인지 고양인지 출력값을 낼 수 없을 것입니다.\n",
    "<br/><br/>\n",
    "\n",
    "그렇기 때문에 우리는 인공 신경망에게 입력 데이터를 주면서 이 데이터는 **고양이**야 라고 알려주어야 하며, 인공 신경망은 이를 통해서\n",
    "\n",
    "내가 낸 출력값과 정답이 얼만큼 차이가 나는지를 계산해서 스스로 가중치의 값을 변화하는 과정을 거치게 됩니다.\n",
    "\n",
    "이러한 학습 방법을 딥러닝에서는 지도 학습(Supervised Learning)이라고 부릅니다.\n",
    "\n",
    "\n",
    "<img src=\"images/cat-dog-flow-horizontal.gif\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 8</u>: Cat and Dog Classifier with Neural Network</center></caption>\n",
    "\n",
    "<br/><br/>\n",
    "그러면 임의로 가중치의 값이 주어진 인공신경망이 입력데이터를 받아서 출력값을 냈을 때, 정답과 출력값이 다른지는 어떻게 알며,\n",
    "\n",
    "얼마나 틀렸는지에 대해서는 어떻게 알까요?\n",
    "\n",
    "이러한 부분에 대해서 중요한 역활을 하는 것이 비용 함수(Cost Function)이 됩니다.\n",
    "\n",
    "그럼 이번에는 인공 신경망에서 쓰이는 비용 함수에 대해서 본격적으로 이야기 해보도록 하겠습니다.\n",
    "\n",
    "<br/><br/><br/>\n",
    "**<font size=\"3\">Cross-Entropy Error** <br/><br/>\n",
    "\n",
    "먼저 해당 스크립트에서는 비용 함수(Cost Function)를 Cross-Entropy Error를 사용합니다.\n",
    "\n",
    "Cross-Entory Error에 대한 수식은 다음과 같습니다.\n",
    "\n",
    "이해가 어려우시다면 뒤에 설명이 나오니 수식이 이렇게 생겼다는 것만 확인하시고, 넘어가시면 됩니다.\n",
    "\n",
    "$$ J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
    "\n",
    "**<font size=\"3\">오차(Error)** <br/><br/>\n",
    "    \n",
    "만약에 인공 신경망이 다음과 같은 2가지 결과를 주었다고 가정해 보겠습니다.\n",
    "\n",
    "우리는 여기서 인공 신경망의 정확도가 몇이고, 얼마나 틀렸는지를 확인해야한다고 합시다\n",
    "<br/><br/><br/>\n",
    "\n",
    "**1-th network**\n",
    "<table style=\"width:50%\", title=\"1-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.3  0.4 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "**2-th network**\n",
    "<table style=\"width:50%\", title=\"2-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.7  0.2 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br/>\n",
    "위 결과에 대해서 3가지 방법(**분류 오차/MSE(Mean Squared Error)/CEE(Cross-Entroy Error)** 오차를 계산해보겠습니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "**1. 분류 오차**\n",
    "\n",
    "분류 오차는 인공 신경망의 출력값들이 정답과 비교했을 때, 몇개를 맞췄고 몇개를 못맞췄는지에 대해서 비교해서\n",
    "\n",
    "인공신경망의 정확도를 측정하는데 쓰입니다.\n",
    "<br/><br/><br/>\n",
    "\n",
    "- *1-th network*:\n",
    " - 분류오차 -> *(3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)*$$\\frac{1}{3} = 0.33$$ <br/><br/><br/> \n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:\n",
    " - 분류오차 -> *(3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)* $$\\frac{1}{3} = 0.33$$ <br/><br/><br/>\n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "\n",
    "두 오차에 대해서 비교해보면, **결과는 같지만**, 2-th 네트워크의 첫 두 샘플은 1-th 네트워크보다 조금 더 확실히 맞추었고, 세번째 샘플은 아깝게 틀렸다는 것을 확인할 수 있습니다.\n",
    "\n",
    "이러한 결과를 보았을 때, **단순 분류 오차**의 계산은 틀린 개수에 대한 결과만 줄 뿐 **정답과 비교해서 얼마나 많이 틀렸는지 얼마나 정확하게 맞았는지** 그 정도에 대한 값을 제공하지 않습니다\n",
    "<br/><br/><br/><br/>\n",
    "**2. MSE(Mean Squared Error)**\n",
    "<br/><br/><br/>\n",
    "MSE(Mean Squared Error)는 정답과 내가 얼마나 멀리 떨어져있는지에 대해서 정의하는 함수입니다.\n",
    "\n",
    "쉽게 이야기하기 위해서 내가 집에 가고 싶은데, 지금 위치에서 집까지의 거리를 확인해서 \n",
    "\n",
    "내가 집까지 가기 위한 거리값을 오차값으로 잡는 것을 MSE로 이해하면됩니다.\n",
    "\n",
    "MSE의 수식은 다음과 같습니다.\n",
    "\n",
    "<br/><br/>\n",
    "$$J = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{(\\hat{Y_{i}} - Y_{i})^{2}}$$\n",
    "\n",
    "<img src=\"images/scatter_plot.gif\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u>Figure 9</u>: Mean Squared Error</center></caption>\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:<br/><br/>\n",
    " - 분류오차 -> $(0.3-0)^{2} + (0.3-0)^{2} + (0.4-1)^{2} = 0.54$ (나머지 2개의 샘플에 대해서는 생략..)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{(0.54 + 0.54 + 1.34)}{3} = 0.81$<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:<br/><br/>\n",
    " - 분류오차 -> (생략...)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{(0.14 + 0.14 + 0.74)}{3} = 0.34$<br/><br/>\n",
    "\n",
    "<br/><br/>\n",
    "**3. Cross-entropy Error**\n",
    "\n",
    "\n",
    "CEE(Cross-Entropy Error)는 내 출력값과 정답값이 얼마나 틀렸는지도 확인하고, 정답이 아닌 부분에 대해서도 이걸 정답으로\n",
    "\n",
    "출력했다면, 이를 같이 반영하는 방법입니다.\n",
    "\n",
    "<br/><br/>\n",
    "수식은 다음과 같습니다. \n",
    "\n",
    "<br/>\n",
    "$$J = -\\frac{1}{m}\\sum\\limits_{i = 1}^{m}(\\hat{Y_{i}}\\log\\left(Y_{i}\\right) + (1-\\hat{Y_{i}})\\log\\left(1- Y_{i}\\right))$$\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:<br/><br/>\n",
    "\n",
    " - 분류오차 -> (나머지 2개의 샘플에 대해서는 생략..)<br/><br/>$ -((0\\times\\log\\left(0.3\\right) + (1-0)\\times\\log\\left(1-0.3\\right)) + 0\\times\\log\\left(0.3\\right) + (1-0)\\times\\log\\left(1-0.3\\right) + 1\\times\\log\\left(0.4\\right) + (1-1)\\times\\log\\left(1-0.4\\right))$<br/><br/>\n",
    " = $(-0.356)+(-0.356)+(-0.916) = -1.628$<br/><br/>\n",
    " \n",
    " - 분류 정확도 -> <br/><br/>\n",
    " $\\frac{-(-2.472 + -3.723 + -1.628)}{3} = 2.607$<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:<br/><br/>\n",
    " - 분류오차 -> (생략...)<br/><br/>\n",
    " - 분류 정확도 -> $\\frac{-(-0.684 + -0.684 + -2.066)}{3} = 1.144$<br/><br/>\n",
    " \n",
    " \n",
    "<br/><br/>\n",
    "결과에서 볼 수 있듯이 MSE는 틀린 샘플에 더 집중하는 특성을 갖습니다.\n",
    "\n",
    "맞은 것과, 틀린 것에 똑같이 집중해야 하는데, 그렇지 않아 비용 함수로 정의할 때, 부적절하다고 할 수 있습니다.\n",
    "\n",
    "따라서, 해당 스크립트의 비용 함수는 Cross-entory Error를 사용합니다.\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Forward Operation\n",
    "\n",
    "1. 먼저 입력 데이터의 차원에 맞게, 가중치 행렬을 생성하고 초기화합니다.\n",
    "2. 초기화된 Weights와 bias를 입력 데이터와 Linear 연산을 통해서 Weights Sum을 진행합니다.\n",
    "3. 그 이후, ACTIVATION (Sigmoid, ReLU etc)를 이용해서 최종 출력을 얻습니다.\n",
    "\n",
    "\n",
    "<img src=\"images/perceptron.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 1</u>: Forward Operation. <br> The model can be summarized as: ***[INPUT -> LINEAR -> ACTIVATION]***</center></caption>\n",
    "\n",
    "<br/>\n",
    "위의 그림을 수식으로 일반화하면, 아래와 같습니다\n",
    "<br/><br/>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Remember that when we compute $W X + b$ in python, it carries out broadcasting. For example, if: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Initialization\n",
    "\n",
    "- 데이터 차원에 맞게 가중치 행렬을 생성하고 초기화합니다.\n",
    "- 일반적으로 weights는 난수로, bias는 0으로 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def basic_initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[ 0.00865408 -0.02301539]]\n",
      "b2 = [[ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = basic_initialize_parameters(2,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756]\n",
    " [-0.00528172 -0.01072969]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.00865408 -0.02301539]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):    \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))        \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Linear Forward Module\n",
    "\n",
    "Linear Forward Module은 아래와 같은 수식으로 일반화되어 계산됩니다.\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "where $A^{[0]} = X$. \n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's dimensions : (28, 28)\n",
      "\n",
      "key of parameters : dict_keys(['b2', 'W1', 'b1', 'W2'])\n",
      "\n",
      "Z[0] = [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -4.94962319e-03  -8.56088005e-03  -7.01055263e-03   9.21223156e-04\n",
      "  -1.40779296e-03   7.28034138e-04  -5.37452855e-05  -5.59312440e-03\n",
      "  -5.34447205e-03   6.72926288e-03  -7.75971334e-03  -1.84216995e-02\n",
      "  -2.45760235e-02  -4.69203890e-02  -3.32101138e-02  -2.23488248e-02\n",
      "  -8.95088092e-03  -5.70976535e-03  -5.02796410e-03  -1.97450375e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "X = train_img[0]\n",
    "print(\"X's dimensions : {}\".format(X.shape), end=\"\\n\\n\")\n",
    "\n",
    "parameters = initialize_parameters_deep([28,28,1])\n",
    "print(\"key of parameters : {}\".format(parameters.keys()), end=\"\\n\\n\")\n",
    "A, W, b = X, parameters[\"W1\"], parameters[\"b1\"]\n",
    "Z, linear_cache = linear_forward(X, W, b)\n",
    "\n",
    "print(\"Z[0] = \" + str(Z[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td> **Z[0]** </td>\n",
    "    <td>[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
    "  -4.94962319e-03  -8.56088005e-03  -7.01055263e-03   9.21223156e-04\n",
    "  -1.40779296e-03   7.28034138e-04  -5.37452855e-05  -5.59312440e-03\n",
    "  -5.34447205e-03   6.72926288e-03  -7.75971334e-03  -1.84216995e-02\n",
    "  -2.45760235e-02  -4.69203890e-02  -3.32101138e-02  -2.23488248e-02\n",
    "  -8.95088092e-03  -5.70976535e-03  -5.02796410e-03  -1.97450375e-03\n",
    "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Linear-Activateion Forward\n",
    "\n",
    "이번 예제에서는 2가지 종류의 Activation function을 사용합니다.\n",
    "\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. `sigmoid` 함수는 **2개**의 return 값을 갖습니다. : activation value \"`a`\", \"\"`Z`\"를 포함하고 있는 `cache`\" (Backpropagation때, 이를 사용합니다). 사용법은 다음과 같습니다: \n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "<img src=\"images/sigmoid.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 2</u>: Sigmoid.</center></caption>\n",
    "\n",
    "- **ReLU**: 수학적 공식은 다음과 같습니다. $A = ReLU(Z) = max(0, Z)$. `relu` 함수는 **2개**의 return값을 갖습니다. : the activation value \"`A`\", `Z`를 포함하고 있는\"`cache`\"\"\" (Backpropagation때, 이를 사용합니다). 사용법은 다음과 같습니다:\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```\n",
    "<img src=\"images/relu.jpeg\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 3</u>: ReLU.</center></caption>\n",
    "\n",
    "- **Derivative of Sigmoid**: <br/> $g'(Z) = \\frac{ d }{ dz }\\frac{ 1 }{ 1+e^{-z} } = \\frac{1}{(1+e^-z)^2}(e^{-z}) = \\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}}) = g(z)(1-g(z))$<br/><br/>\n",
    "\n",
    "- **Derivative of ReLU**: <br/> $g'(Z) = \\max(0, Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(Z):\n",
    "    return (1/(1+np.exp(-Z)) * (1-1/(1+np.exp(-Z))))\n",
    "\n",
    "def derivative_relu(Z):\n",
    "    np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z)), derivative_sigmoid(Z)\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z), derivative_relu(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must not be None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0c9d00138fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ReLU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderivate_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"derivative ReLU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/edu/env/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/edu/env/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/edu/env/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/edu/env/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# downstream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must not be None"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAJPCAYAAABGnGG7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJJWDokiRJKgFDlyRJUgn8P8gPYdh6Tk6gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cfc4774a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = [round(i*0.01, 5) for i in range(-1000,1000)]\n",
    "x = np.asarray(tmp)\n",
    "\n",
    "f, axarr = plt.subplots(4, sharex=True)\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(10)\n",
    "sig, derivate_sig = sigmoid(x)\n",
    "ReLU, derivate_relu = relu(x)\n",
    "axarr[0].plot(x, sig)\n",
    "axarr[0].set_title(\"Sigmoid\")\n",
    "axarr[1].plot(x, derivate_sig)\n",
    "axarr[1].set_title(\"derivative Sigmoid\")\n",
    "axarr[2].plot(x, ReLU)\n",
    "axarr[2].set_title(\"ReLU\")\n",
    "axarr[3].plot(x, derivate_relu)\n",
    "axarr[3].set_title(\"derivative ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's dimensions : (28, 28)\n",
      "\n",
      "key of parameters : dict_keys(['b2', 'W1', 'b1', 'W2'])\n",
      "\n",
      "With sigmoid: A[0] = [ 0.5         0.5         0.5         0.5         0.4987626   0.49785979\n",
      "  0.49824737  0.50023031  0.49964805  0.50018201  0.49998656  0.49860172\n",
      "  0.49866389  0.50168231  0.49806008  0.49539471  0.4938563   0.48827205\n",
      "  0.49169823  0.49441303  0.49776229  0.49857256  0.49874301  0.49950637\n",
      "  0.5         0.5         0.5         0.5       ]\n",
      "With ReLU: A[0] = [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.00092122  0.          0.00072803  0.          0.          0.\n",
      "  0.00672926  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "X = train_img[0]\n",
    "print(\"X's dimensions : {}\".format(X.shape), end=\"\\n\\n\")\n",
    "\n",
    "parameters = initialize_parameters_deep([28,28,1])\n",
    "print(\"key of parameters : {}\".format(parameters.keys()), end=\"\\n\\n\")\n",
    "A_prev, W, b = X, parameters[\"W1\"], parameters[\"b1\"]\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"sigmoid\")\n",
    "print(\"With sigmoid: A[0] = \" + str(A[0]))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "print(\"With ReLU: A[0] = \" + str(A[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A[0] ** </td>\n",
    "    <td > [ 0.5         0.5         0.5         0.5         0.4987626   0.49785979\n",
    "  0.49824737  0.50023031  0.49964805  0.50018201  0.49998656  0.49860172\n",
    "  0.49866389  0.50168231  0.49806008  0.49539471  0.4938563   0.48827205\n",
    "  0.49169823  0.49441303  0.49776229  0.49857256  0.49874301  0.49950637\n",
    "  0.5         0.5         0.5         0.5       ]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A[0] ** </td>\n",
    "    <td > [ 0.          0.          0.          0.          0.          0.          0.\n",
    "  0.00092122  0.          0.00072803  0.          0.          0.\n",
    "  0.00672926  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.          0.\n",
    "  0.        ]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. L-Layer Model\n",
    "\n",
    "이전까지 간단한 Forward Operation이 어떻게 구성되는지 확인하였습니다.<br/>\n",
    "이러한 Forward Operation을 여러번 진행하게되면, 이것이 Deep Neural Network가 됩니다.<br/>\n",
    "\n",
    "이번 코드에서는 SIGMOID ACTIVATION을 이용한 Muli Layer Forward Operation에 대해서 알아보겠습니다.\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u>Figure 4</u>: L-Layer Model.</center></caption>\n",
    "\n",
    "**Note**: 이번 코드에서 다시 한번 Linear-Activation 수식을 언급하겠습니다.<br/>\n",
    "$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (마지막 출력의 경우 출력값의 표현을 `Yhat`이라고 합니다, i.e., 수식적으로 표현하면 다음과 같습니다. $\\hat{Y}$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, \n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)], \n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[ 0.5         0.5         0.5         0.5         0.50000488  0.49999259\n",
      "   0.49999292  0.50006658  0.50019038  0.50018837  0.5000768   0.50000425\n",
      "   0.50000441  0.49996328  0.49990328  0.49989251  0.49988565  0.50000674\n",
      "   0.49992422  0.49995059  0.49978186  0.49970537  0.4997401   0.49989355\n",
      "   0.5         0.5         0.5         0.5       ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = train_img[0], initialize_parameters_deep([28,28,1])\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.5         0.5         0.5         0.5         0.50000488  0.49999259\n",
    "   0.49999292  0.50006658  0.50019038  0.50018837  0.5000768   0.50000425\n",
    "   0.50000441  0.49996328  0.49990328  0.49989251  0.49988565  0.50000674\n",
    "   0.49992422  0.49995059  0.49978186  0.49970537  0.4997401   0.49989355\n",
    "   0.5         0.5         0.5         0.5       ]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 2</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Cost Function\n",
    "\n",
    "지금까지 Muli Layer Forward Operation을 구현하였습니다.\n",
    "\n",
    "이번엔 Forward Operation의 결과와 label 값을 통해서 나온 결과를 비교해서 비용함수를 정의하도록 하겠습니다.\n",
    "\n",
    "**Exercise**: 여기서는 cross-entropy cost $J$를 계산합니다, 수식은 다음과 같습니다.: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
    "\n",
    "**Why Cross-entropy Cost?**: MSE(Mean Squared Error)의 경우네는 틀린 샘플에 대해서 더 집중하는 특징을 갖습니다. 틀린 것과 맞은 것에 대해서 똑같이 집중해서 Loss를 전파해줘야하는데, 틀린 부분에 대해서만 Loss를 전파하기 때문에 학습이 덜 되는 경향이 있습니다\n",
    "\n",
    "만약에 네트워크가 다음과 같은 2가지 결과를 주었다고 가정해 보겠습니다.\n",
    "\n",
    "**1-th network**\n",
    "<table style=\"width:50%\", title=\"1-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.3  0.4 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "**2-th network**\n",
    "<table style=\"width:50%\", title=\"2-th network\">\n",
    "  <tr>\n",
    "    <td> **계산결과** </td>\n",
    "    <td> **라벨(A/B/C)** </td>\n",
    "    <td> **Correct?** </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.2  0.7 </td>\n",
    "    <td > 0  0  1 (A) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.1  0.7  0.2 </td>\n",
    "    <td > 0  1  0 (B) </td> \n",
    "    <td > yes </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> 0.3  0.4  0.3 </td>\n",
    "    <td > 1  0  0 (C) </td> \n",
    "    <td > no </td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "위 결과에 대해서 오차를 계산해보겠습니다.\n",
    "\n",
    "**1. 분류 오차**\n",
    "- *1-th network*:\n",
    " - 분류오차 -> $$\\frac{1}{3} = 0.33$$ (3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)\n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:\n",
    " - 분류오차 -> $$\\frac{1}{3} = 0.33$$ (3개의 샘플 중에 1개가 라벨과 일치하지 않으므로)\n",
    " - 분류 정확도 -> $$\\frac{2}{3} = 0.67$$\n",
    "\n",
    "두 오차에 대해서 비교해보면, 결과는 같지만, 2-th 네트워크의 첫 두 샘플은 1-th 네트워크보다 조금 더 확실히 맞추었고, 세번째 샘플은 아깝게 틀렸습니다.\n",
    "\n",
    "이러한 결과를 보았을 때, 단순 분류 오차의 계산은 틀린 개수에 대한 결과만 줄 뿐 label과 비교해서 얼마나 많이 틀렸는지 얼마나 정확하게 맞았는지 그 정도에 대한 값을 제공하지 않습니다\n",
    "\n",
    "**2. MSE(Mean Squared Error)**\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n}{(\\hat{Y_{i}} - Y_{i})^{2}}$\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:\n",
    " - 분류오차 -> $(0.3-0)^{2} + (0.3-0)^{2} + (0.4-1)^{2} = 0.54$ (나머지 2개의 샘플에 대해서는 생략..)\n",
    " - 분류 정확도 -> $\\frac{(0.54 + 0.54 + 1.34)}{3} = 0.81$\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:\n",
    " - 분류오차 -> (생략...)\n",
    " - 분류 정확도 -> $\\frac{(0.14 + 0.14 + 0.74)}{3} = 0.34$\n",
    "\n",
    "**3. Cross-entropy Error**\n",
    "\n",
    "$-\\frac{1}{m}\\sum\\limits_{i = 1}^{m}(\\hat{Y_{i}}\\log\\left(Y_{i}\\right) + (1-\\hat{Y_{i}})\\log\\left(1- Y_{i}\\right))$\n",
    "\n",
    "<br/>\n",
    "- *1-th network*:\n",
    " - 분류오차 -> $(0.3-0)^{2} + (0.3-0)^{2} + (0.4-1)^{2} = 0.54$ (나머지 2개의 샘플에 대해서는 생략..)\n",
    " - 분류 정확도 -> $\\frac{(0.54 + 0.54 + 1.34)}{3} = 0.81$\n",
    "<br/><br/>\n",
    "\n",
    "- *2-th network*:\n",
    " - 분류오차 -> (생략...)\n",
    " - 분류 정확도 -> $\\frac{(0.14 + 0.14 + 0.74)}{3} = 0.34$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
